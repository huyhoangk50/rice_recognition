{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import glob\n",
    "from scipy import sparse\n",
    "import random as rd\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "import keras\n",
    "import pandas as pd\n",
    "import csv\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import sklearn.preprocessing as prep\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import sklearn.metrics as metrics\n",
    "import csv\n",
    "import datetime\n",
    "min_max_scaler = preprocessing.MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constant variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasetPath = '/media/hoangnh/TaiLieu/xulianh/NhanDienHatThoc/Dataset/final_feature_data/'\n",
    "groupDatasetName =['dataset-G1.mat', 'dataset-G2.mat','dataset-G3.mat','dataset-G4.mat']\n",
    "# groupDatasetName =['dataset-G1.mat']\n",
    "nGroup = len (groupDatasetName)\n",
    "performancePath = './Result/performances/'\n",
    "pcaMatrixPath = '/media/hoangnh/TaiLieu/xulianh/NhanDienHatThoc/Dataset/Model/PCAAll.mat'\n",
    "nTime = 5\n",
    "nNegTrainSample = 16 #number of negative sample in each species when training\n",
    "nPosTrainSample = 80 #number of positive sample in each species when training\n",
    "nPosValSample = 16\n",
    "nValSample = 16\n",
    "nNegValSample = 16\n",
    "nTrainSample = 80\n",
    "nFeature = 262\n",
    "nSpecFeature = 256\n",
    "nSpatialFeatue = 6\n",
    "nSpecies = 6\n",
    "nPca = 50\n",
    "max_features = nPca + nSpatialFeatue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize feature vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------normarlize spatial data---------------\n",
    "def normalize(trainSet, valSet):\n",
    "    allSpatialData= np.asarray([])\n",
    "    for i in range(0,nSpecies):\n",
    "        if allSpatialData.shape[0] ==0:\n",
    "            allSpatialData = trainSet[i][:, nSpecFeature:nFeature]\n",
    "            allSpatialData = np.concatenate((allSpatialData, valSet[i][:, nSpecFeature:nFeature]), axis = 0)\n",
    "        else:\n",
    "            allSpatialData = np.concatenate((allSpatialData, trainSet[i][:, nSpecFeature:nFeature]), axis = 0)\n",
    "            allSpatialData = np.concatenate((allSpatialData, valSet[i][:, nSpecFeature:nFeature]), axis = 0)\n",
    "    allSpatialData = min_max_scaler.fit_transform(allSpatialData)\n",
    "\n",
    "    print allSpatialData.shape\n",
    "\n",
    "    for i in range(0,nSpecies):\n",
    "        trainSet[i][:, nSpecFeature:nFeature] = allSpatialData[i*(nPosTrainSample + nPosValSample):i*(nPosTrainSample + nPosValSample) +nPosTrainSample,:]\n",
    "        valSet[i][:, nSpecFeature:nFeature] = allSpatialData[i*(nPosTrainSample + nPosValSample) + nPosTrainSample:(i+1)*(nPosTrainSample + nPosValSample),:]\n",
    "    return trainSet,valSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defind classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# use pca matrix to transfrom spec feature to low-dimenson vector\n",
    "# firstly, separate spec and spatial feature\n",
    "# than use pca matrix to transform spec feature\n",
    "# finally combine transformed spec feature and original spatial feature to unique vector feature.\n",
    "\n",
    "def pcaTransform(data, n_features):\n",
    "    pcaContent = sio.loadmat(pcaMatrixPath)\n",
    "    pcaMatrix =  np.matrix(pcaContent[\"prinCompMat\"])\n",
    "    specData = data[:, 0:nSpecFeature]\n",
    "    specMatrix = np.matrix(specData)\n",
    "    spatialData = data[:, nSpecFeature:nFeature]\n",
    "    trainsformedSpecData = specData * pcaMatrix[:,0:n_features]\n",
    "    data = np.concatenate((trainsformedSpecData, spatialData), axis = 1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = [\"Random Forest\", \"Neural Net\", \"AdaBoost\"]\n",
    "\n",
    "classifiers = [\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=500, max_features = max_features),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RFC = RandomForestClassifier(max_depth=10, n_estimators=500, max_features = max_features)\n",
    "NNC = MLPClassifier(alpha=1)\n",
    "ABC = AdaBoostClassifier(n_estimators=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check performance functon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def checkPerformance(clf, trainData, trainLabels, valData, valLabels):\n",
    "    clf.fit(trainData, trainLabels)\n",
    "    pridictLabels = clf.predict(valData)\n",
    "    accuracy = clf.score(valData, valLabels)\n",
    "    precision = metrics.precision_score(allValLabels, pridictLabels, pos_label = 1)\n",
    "    recall = metrics.recall_score(allValLabels, pridictLabels, pos_label = 1)\n",
    "    confusionMatrix = metrics.confusion_matrix(y_true=allValLabels, y_pred=pridictLabels)\n",
    "    specificity = confusionMatrix[1][1]*1.0/(confusionMatrix[1][0] + confusionMatrix[1][1])\n",
    "    return np.asarray([recall, precision, accuracy, specificity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write performance to csv file function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeToCsvFile(fileNames, data):\n",
    "    temp = []\n",
    "    for i in range(0, nGroup):\n",
    "        g = data[i]\n",
    "        temp.append(g.reshape(nSpecies*nTime, 3).T)\n",
    "    temp = np.asarray(temp)\n",
    "    data = temp.reshape(nGroup * 3, nSpecies*nTime)\n",
    "    with open(fileNames, 'wb') as dictFile:\n",
    "        writer = csv.writer(dictFile)\n",
    "        writer.writerow(groupDatasetName)\n",
    "        writer.writerows(data.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['NDC1' 'NV1' 'NepCT' 'NepBH' 'NTHY' 'NDSLH']\n",
      " ['BC15' 'KC111' 'NBK' 'NBP' 'NPT1' 'TB13']\n",
      " ['CL61' 'PD211' 'R068' 'SHPT1' 'SVN1' 'NBP']\n",
      " ['NT16' 'NT16' 'R068' 'SHPT1' 'SVN1' 'NBP']]\n"
     ]
    }
   ],
   "source": [
    "g1Names = ['NDC1', 'NV1','NepCT','NepBH', 'NTHY','NDSLH']\n",
    "g2Names = ['BC15', 'KC111', 'NBK', 'NBP', 'NPT1','TB13']\n",
    "g3Names = ['CL61', 'PD211', 'R068', 'SHPT1', 'SVN1', 'NBP']\n",
    "g4Names = ['NT16', 'NT16', 'R068', 'SHPT1', 'SVN1', 'NBP']\n",
    "speciesNames = []\n",
    "speciesNames= np.asarray([g1Names, g2Names, g3Names, g4Names])\n",
    "print speciesNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 0], [5, 3], [1, 2]]\n",
      "[array(['NTHY', 'NDC1'],\n",
      "      dtype='|S5'), array(['NDSLH', 'NepBH'],\n",
      "      dtype='|S5'), array(['NV1', 'NepCT'],\n",
      "      dtype='|S5')]\n",
      "[[1, 3, 0], [4, 5, 2]]\n",
      "[array(['NV1', 'NepBH', 'NDC1'],\n",
      "      dtype='|S5'), array(['NTHY', 'NDSLH', 'NepCT'],\n",
      "      dtype='|S5')]\n",
      "[[5, 3, 2, 4]]\n",
      "[array(['NDSLH', 'NepBH', 'NepCT', 'NTHY'],\n",
      "      dtype='|S5')]\n",
      "[[1, 2, 0, 3, 4]]\n",
      "[array(['NV1', 'NepCT', 'NDC1', 'NepBH', 'NTHY'],\n",
      "      dtype='|S5')]\n"
     ]
    }
   ],
   "source": [
    "title = ''\n",
    "group = 0\n",
    "for partSize in range(2, 6):\n",
    "    nPart = nSpecies/partSize\n",
    "    sample = range(0, nSpecies)\n",
    "    shuffle(sample)\n",
    "    partArray =[]\n",
    "    nameArray = []\n",
    "    for part in range(0,nPart):\n",
    "        partArray.append(sample[part*partSize: (part + 1)* partSize])\n",
    "        nameArray.append(speciesNames[group][sample[part*partSize: (part + 1)* partSize]])\n",
    "    print partArray\n",
    "    print nameArray\n",
    "\n",
    "#     speciesNameArray = \n",
    "# for group in range(3, 4):\n",
    "#     dataFileName = groupDatasetName[group]\n",
    "#     print 'group ' + dataFileName\n",
    "#     content = sio.loadmat(datasetPath + dataFileName)\n",
    "#     species = np.asarray(content['dataset']['species'][0][0][0])\n",
    "#     print species"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check performance based on classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group dataset-G1.mat\n",
      "train shape (6,) train feature shape (80, 262)\n",
      "val shape (6,)\n",
      "(576, 6)\n",
      "++++++++++ 1 th species++++++++++++++\n",
      "--------time 0 ---------\n",
      "recalls [ 0.9375  0.9375  0.875 ]\n",
      "precisions [ 0.46875     0.42857143  0.42424242]\n",
      "accuracies [ 0.8125   0.78125  0.78125]\n",
      "specificities [ 0.9375  0.9375  0.875 ]\n",
      "--------time 1 ---------\n",
      "recalls [ 0.9375  0.9375  0.875 ]\n",
      "precisions [ 0.51724138  0.44117647  0.4375    ]\n",
      "accuracies [ 0.84375     0.79166667  0.79166667]\n",
      "specificities [ 0.9375  0.9375  0.875 ]\n",
      "--------time 2 ---------\n",
      "recalls [ 0.9375  0.9375  0.875 ]\n",
      "precisions [ 0.625       0.44117647  0.48275862]\n",
      "accuracies [ 0.89583333  0.79166667  0.82291667]\n",
      "specificities [ 0.9375  0.9375  0.875 ]\n",
      "--------time 3 ---------\n",
      "recalls [ 0.875   0.9375  0.875 ]\n",
      "precisions [ 0.58333333  0.46875     0.4516129 ]\n",
      "accuracies [ 0.875       0.8125      0.80208333]\n",
      "specificities [ 0.875   0.9375  0.875 ]\n",
      "--------time 4 ---------\n",
      "recalls [ 0.9375  0.9375  0.875 ]\n",
      "precisions [ 0.48387097  0.48387097  0.46666667]\n",
      "accuracies [ 0.82291667  0.82291667  0.8125    ]\n",
      "specificities [ 0.9375  0.9375  0.875 ]\n",
      "++++++++++ 2 th species++++++++++++++\n",
      "--------time 0 ---------\n",
      "recalls [ 0.875   0.8125  0.9375]\n",
      "precisions [ 0.77777778  0.61904762  0.83333333]\n",
      "accuracies [ 0.9375      0.88541667  0.95833333]\n",
      "specificities [ 0.875   0.8125  0.9375]\n",
      "--------time 1 ---------\n",
      "recalls [ 0.9375  0.875   0.9375]\n",
      "precisions [ 0.75        0.63636364  0.68181818]\n",
      "accuracies [ 0.9375      0.89583333  0.91666667]\n",
      "specificities [ 0.9375  0.875   0.9375]\n",
      "--------time 2 ---------\n",
      "recalls [ 0.75   0.75   0.875]\n",
      "precisions [ 0.66666667  0.57142857  0.875     ]\n",
      "accuracies [ 0.89583333  0.86458333  0.95833333]\n",
      "specificities [ 0.75   0.75   0.875]\n",
      "--------time 3 ---------\n",
      "recalls [ 0.875   0.8125  0.875 ]\n",
      "precisions [ 0.7         0.72222222  0.66666667]\n",
      "accuracies [ 0.91666667  0.91666667  0.90625   ]\n",
      "specificities [ 0.875   0.8125  0.875 ]\n",
      "--------time 4 ---------\n",
      "recalls [ 0.875  0.75   0.875]\n",
      "precisions [ 0.82352941  0.52173913  0.82352941]\n",
      "accuracies [ 0.94791667  0.84375     0.94791667]\n",
      "specificities [ 0.875  0.75   0.875]\n",
      "++++++++++ 3 th species++++++++++++++\n",
      "--------time 0 ---------\n",
      "recalls [ 1.      0.9375  1.    ]\n",
      "precisions [ 0.59259259  0.5         0.61538462]\n",
      "accuracies [ 0.88541667  0.83333333  0.89583333]\n",
      "specificities [ 1.      0.9375  1.    ]\n",
      "--------time 1 ---------\n",
      "recalls [ 1.      0.9375  1.    ]\n",
      "precisions [ 0.64        0.51724138  0.61538462]\n",
      "accuracies [ 0.90625     0.84375     0.89583333]\n",
      "specificities [ 1.      0.9375  1.    ]\n",
      "--------time 2 ---------\n",
      "recalls [ 0.875   0.9375  1.    ]\n",
      "precisions [ 0.63636364  0.53571429  0.66666667]\n",
      "accuracies [ 0.89583333  0.85416667  0.91666667]\n",
      "specificities [ 0.875   0.9375  1.    ]\n",
      "--------time 3 ---------\n",
      "recalls [ 0.9375  0.9375  1.    ]\n",
      "precisions [ 0.53571429  0.42857143  0.64      ]\n",
      "accuracies [ 0.85416667  0.78125     0.90625   ]\n",
      "specificities [ 0.9375  0.9375  1.    ]\n",
      "--------time 4 ---------\n",
      "recalls [ 1.      0.9375  0.9375]\n",
      "precisions [ 0.59259259  0.45454545  0.6       ]\n",
      "accuracies [ 0.88541667  0.80208333  0.88541667]\n",
      "specificities [ 1.      0.9375  0.9375]\n",
      "++++++++++ 4 th species++++++++++++++\n",
      "--------time 0 ---------\n",
      "recalls [ 0.6875  0.6875  0.8125]\n",
      "precisions [ 0.40740741  0.40740741  0.40625   ]\n",
      "accuracies [ 0.78125     0.78125     0.77083333]\n",
      "specificities [ 0.6875  0.6875  0.8125]\n",
      "--------time 1 ---------\n",
      "recalls [ 0.8125  0.6875  0.9375]\n",
      "precisions [ 0.48148148  0.39285714  0.42857143]\n",
      "accuracies [ 0.82291667  0.77083333  0.78125   ]\n",
      "specificities [ 0.8125  0.6875  0.9375]\n",
      "--------time 2 ---------\n",
      "recalls [ 0.75    0.6875  0.8125]\n",
      "precisions [ 0.46153846  0.39285714  0.37142857]\n",
      "accuracies [ 0.8125      0.77083333  0.73958333]\n",
      "specificities [ 0.75    0.6875  0.8125]\n",
      "--------time 3 ---------\n",
      "recalls [ 0.75    0.6875  0.875 ]\n",
      "precisions [ 0.44444444  0.40740741  0.46666667]\n",
      "accuracies [ 0.80208333  0.78125     0.8125    ]\n",
      "specificities [ 0.75    0.6875  0.875 ]\n",
      "--------time 4 ---------\n",
      "recalls [ 0.6875  0.6875  0.625 ]\n",
      "precisions [ 0.40740741  0.37931034  0.35714286]\n",
      "accuracies [ 0.78125     0.76041667  0.75      ]\n",
      "specificities [ 0.6875  0.6875  0.625 ]\n",
      "++++++++++ 5 th species++++++++++++++\n",
      "--------time 0 ---------\n",
      "recalls [ 0.875  1.     1.   ]\n",
      "precisions [ 0.7         0.66666667  0.64      ]\n",
      "accuracies [ 0.91666667  0.91666667  0.90625   ]\n",
      "specificities [ 0.875  1.     1.   ]\n",
      "--------time 1 ---------\n",
      "recalls [ 0.9375  1.      1.    ]\n",
      "precisions [ 0.75        0.64        0.76190476]\n",
      "accuracies [ 0.9375      0.90625     0.94791667]\n",
      "specificities [ 0.9375  1.      1.    ]\n",
      "--------time 2 ---------\n",
      "recalls [ 0.9375  1.      1.    ]\n",
      "precisions [ 0.6         0.64        0.84210526]\n",
      "accuracies [ 0.88541667  0.90625     0.96875   ]\n",
      "specificities [ 0.9375  1.      1.    ]\n",
      "--------time 3 ---------\n",
      "recalls [ 0.875   1.      0.9375]\n",
      "precisions [ 0.63636364  0.59259259  0.78947368]\n",
      "accuracies [ 0.89583333  0.88541667  0.94791667]\n",
      "specificities [ 0.875   1.      0.9375]\n",
      "--------time 4 ---------\n",
      "recalls [ 0.9375  1.      1.    ]\n",
      "precisions [ 0.68181818  0.53333333  0.76190476]\n",
      "accuracies [ 0.91666667  0.85416667  0.94791667]\n",
      "specificities [ 0.9375  1.      1.    ]\n",
      "++++++++++ 6 th species++++++++++++++\n",
      "--------time 0 ---------\n",
      "recalls [ 0.875   0.6875  0.9375]\n",
      "precisions [ 0.4375      0.28947368  0.68181818]\n",
      "accuracies [ 0.79166667  0.66666667  0.91666667]\n",
      "specificities [ 0.875   0.6875  0.9375]\n",
      "--------time 1 ---------\n",
      "recalls [ 0.6875  0.5     0.9375]\n",
      "precisions [ 0.47826087  0.22857143  0.68181818]\n",
      "accuracies [ 0.82291667  0.63541667  0.91666667]\n",
      "specificities [ 0.6875  0.5     0.9375]\n",
      "--------time 2 ---------\n",
      "recalls [ 0.75   0.625  0.875]\n",
      "precisions [ 0.57142857  0.27777778  0.56      ]\n",
      "accuracies [ 0.86458333  0.66666667  0.86458333]\n",
      "specificities [ 0.75   0.625  0.875]\n",
      "--------time 3 ---------\n",
      "recalls [ 0.75    0.625   0.9375]\n",
      "precisions [ 0.57142857  0.29411765  0.53571429]\n",
      "accuracies [ 0.86458333  0.6875      0.85416667]\n",
      "specificities [ 0.75    0.625   0.9375]\n",
      "--------time 4 ---------\n",
      "recalls [ 0.75    0.6875  0.9375]\n",
      "precisions [ 0.46153846  0.31428571  0.6       ]\n",
      "accuracies [ 0.8125      0.69791667  0.88541667]\n",
      "specificities [ 0.75    0.6875  0.9375]\n",
      "2018-01-10 16:06:36.127750\n"
     ]
    }
   ],
   "source": [
    "groupBasedRecalls = []\n",
    "groupBasedPrecisions = []\n",
    "groupBasedAccuracies = []\n",
    "groupBasedSpecificities = []\n",
    "for group in range(0, nGroup):\n",
    "    dataFileName = groupDatasetName[group]\n",
    "    print 'group ' + dataFileName\n",
    "    content = sio.loadmat(datasetPath + dataFileName)\n",
    "    trainSet = np.asarray(content['dataset']['train'][0][0][0])\n",
    "    print 'train shape', trainSet.shape, 'train feature shape', trainSet[0].shape\n",
    "    valSet = np.asarray(content['dataset']['valid'])[0][0][0]\n",
    "    print 'val shape', valSet.shape\n",
    "    species = np.asarray(content['dataset']['species'])[0][0][0]\n",
    "    trainSet, valSet = normalize(trainSet, valSet)\n",
    "#     nSpecies = species.shape[0]\n",
    "\n",
    "    speciesBasedRecalls = []\n",
    "    speciesBasedPrecisions = []\n",
    "    speciesBasedAccuracies = []\n",
    "    speciesBasedSpecificities = []\n",
    "    for i in range(0, nSpecies):\n",
    "        print '++++++++++', str(i+1), 'th species' '++++++++++++++'\n",
    "#       generate training pos sample\n",
    "        posSamples = trainSet[i]\n",
    "        nPosSample = posSamples.shape[0]\n",
    "        posLabels = np.ones(nPosSample, dtype=np.int)\n",
    "        negLabels = np.zeros(nNegTrainSample*(nSpecies-1), dtype=np.int)\n",
    "        \n",
    "#       generate validate pos sample\n",
    "        valSamples = valSet[i]\n",
    "        posValSamples = valSet[i]\n",
    "        posValLabels = np.ones(nPosValSample, dtype=np.int)\n",
    "        negValLabels = np.zeros(nNegValSample*(nSpecies-1), dtype=np.int)\n",
    "\n",
    "        timeBasedRecalls = np.asarray([])\n",
    "        timeBasedPrecisions = np.asarray([])\n",
    "        timeBasedAccuracies = np.asarray([])\n",
    "        timeBasedSpecificities = np.asarray([])\n",
    "        for time in range(0, nTime):\n",
    "            print '--------time', time, '---------'\n",
    "            negTempSamples = []\n",
    "            #get train negagtive sample in the rest species\n",
    "            for restSpecies in range(0, i) + range(i+1, nSpecies):\n",
    "                randSample = range(0, nTrainSample)\n",
    "                shuffle(randSample)\n",
    "                negTempSamples.append(trainSet[restSpecies][randSample[0:nNegTrainSample], :])\n",
    "            negSamples = np.asarray(negTempSamples)\n",
    "            negSamples = negSamples.reshape((nSpecies-1) * nNegTrainSample, nFeature)\n",
    "            \n",
    "            allTrainData = np.concatenate((posSamples, negSamples), axis = 0)\n",
    "            allTrainLabels = np.concatenate((posLabels, negLabels), axis = 0)\n",
    "            \n",
    "            #         generate validate neg samples\n",
    "            negTempValSamples= []\n",
    "            for restSpecies in range(0, i) + range(i+1, nSpecies):\n",
    "                randSample = range(0, nValSample)\n",
    "                shuffle(randSample)\n",
    "                negTempValSamples.append(valSet[restSpecies][randSample[0:nNegValSample], :])\n",
    "            \n",
    "            negValSamples = np.asarray(negTempValSamples)\n",
    "            negValSamples=negValSamples.reshape((nSpecies-1)*nNegValSample, nFeature)\n",
    "            allValData = np.concatenate((posValSamples, negValSamples), axis = 0)\n",
    "            allValLabels = np.concatenate((posValLabels, negValLabels), axis = 0)\n",
    "\n",
    "            \n",
    "            \n",
    "            transformedTrainData = pcaTransform(allTrainData, n_features= nPca)\n",
    "            transformedValData = pcaTransform(allValData, n_features= nPca)\n",
    "            \n",
    "#             print 'allTrainData shape', allTrainData.shape\n",
    "            \n",
    "            rfcPerformance = checkPerformance(RFC, transformedTrainData, allTrainLabels, transformedValData, allValLabels)\n",
    "            nncPerformance = checkPerformance(NNC, transformedTrainData, allTrainLabels, transformedValData, allValLabels)\n",
    "            abcPerformance = checkPerformance(ABC, transformedTrainData, allTrainLabels, transformedValData, allValLabels)\n",
    "            \n",
    "            recalls = np.asarray([rfcPerformance[0], nncPerformance[0], abcPerformance[0]])\n",
    "            precisions = np.asarray([rfcPerformance[1], nncPerformance[1], abcPerformance[1]])\n",
    "            accuracies = np.asarray([rfcPerformance[2], nncPerformance[2], abcPerformance[2]])\n",
    "            specificities = np.asarray([rfcPerformance[3], nncPerformance[3], abcPerformance[3]])\n",
    "            print \"recalls\", recalls\n",
    "            print \"precisions\", precisions\n",
    "            print \"accuracies\", accuracies\n",
    "            print \"specificities\", specificities\n",
    "            if timeBasedRecalls.shape[0] == 0:\n",
    "                timeBasedRecalls = recalls\n",
    "                timeBasedPrecisions = precisions\n",
    "                timeBasedAccuracies = accuracies\n",
    "                timeBasedSpecificities = specificities\n",
    "            else:\n",
    "                timeBasedRecalls = np.concatenate((timeBasedRecalls, recalls), axis = 0)\n",
    "                timeBasedPrecisions = np.concatenate((timeBasedPrecisions, precisions), axis = 0)\n",
    "                timeBasedAccuracies = np.concatenate((timeBasedAccuracies, accuracies), axis = 0)\n",
    "                timeBasedSpecificities = np.concatenate((timeBasedSpecificities, specificities), axis = 0)\n",
    "            \n",
    "        timeBasedRecalls = timeBasedRecalls.reshape(nTime, 3)\n",
    "        timeBasedPrecisions = timeBasedPrecisions.reshape(nTime, 3)\n",
    "        timeBasedAccuracies = timeBasedAccuracies.reshape(nTime, 3)\n",
    "        timeBasedSpecificities = timeBasedSpecificities.reshape(nTime, 3)\n",
    "\n",
    "        speciesBasedRecalls.append(timeBasedRecalls)\n",
    "        speciesBasedPrecisions.append(timeBasedPrecisions)\n",
    "        speciesBasedAccuracies.append(timeBasedAccuracies)\n",
    "        speciesBasedSpecificities.append(timeBasedAccuracies)\n",
    "    speciesBasedRecalls = np.asarray(speciesBasedRecalls)\n",
    "    speciesBasedPrecisions = np.asarray(speciesBasedPrecisions)\n",
    "    speciesBasedAccuracies = np.asarray(speciesBasedAccuracies)\n",
    "    speciesBasedSpecificities = np.asarray(speciesBasedSpecificities)\n",
    "    \n",
    "    speciesBasedRecalls = speciesBasedRecalls.reshape(nSpecies, nTime, 3)\n",
    "    speciesBasedPrecisions = speciesBasedPrecisions.reshape(nSpecies, nTime, 3)\n",
    "    speciesBasedAccuracies = speciesBasedAccuracies.reshape(nSpecies, nTime, 3)\n",
    "    speciesBasedSpecificities = speciesBasedSpecificities.reshape(nSpecies, nTime, 3)\n",
    "    \n",
    "    groupBasedRecalls.append(speciesBasedRecalls)\n",
    "    groupBasedPrecisions.append(speciesBasedPrecisions)\n",
    "    groupBasedAccuracies.append(speciesBasedAccuracies)\n",
    "    groupBasedSpecificities.append(speciesBasedSpecificities)\n",
    "    \n",
    "groupBasedRecalls = np.asarray(groupBasedRecalls)\n",
    "groupBasedPrecisions = np.asarray(groupBasedPrecisions)\n",
    "groupBasedAccuracies = np.asarray(groupBasedAccuracies)\n",
    "groupBasedSpecificities = np.asarray(groupBasedSpecificities)\n",
    "\n",
    "groupBasedRecalls = groupBasedRecalls.reshape(nGroup, nSpecies, nTime, 3)\n",
    "groupBasedPrecisions= groupBasedPrecisions.reshape(nGroup, nSpecies, nTime, 3)\n",
    "groupBasedAccuracies = groupBasedAccuracies.reshape(nGroup, nSpecies, nTime, 3)\n",
    "groupBasedSpecificities = groupBasedSpecificities.reshape(nGroup, nSpecies, nTime, 3)\n",
    "# save to npy files\n",
    "currentDT = datetime.datetime.now()\n",
    "print (str(currentDT))\n",
    "np.save(performancePath +'precisions' + str(currentDT), groupBasedPrecisions)\n",
    "np.save(performancePath +'recalls'+ str(currentDT), groupBasedRecalls)\n",
    "np.save(performancePath +'accuracies'+ str(currentDT), groupBasedAccuracies)\n",
    "np.save(performancePath +'specifitities'+ str(currentDT), groupBasedSpecificities)\n",
    "\n",
    "# save to csv files\n",
    "writeToCsvFile('accuracy'+ str(currentDT) +'.csv', groupBasedAccuracies)\n",
    "writeToCsvFile('recall'+ str(currentDT) +'.csv', groupBasedRecalls)\n",
    "writeToCsvFile('precision'+ str(currentDT) +'.csv', groupBasedPrecisions)\n",
    "writeToCsvFile('specifitities'+ str(currentDT) +'.csv', groupBasedSpecificities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "[[3 1]\n",
      " [4 2]]\n",
      "0.428571428571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "y_pred = [0, 0, 1, 0, 0, 1, 0, 0, 1, 0]\n",
    "con = confusion_matrix(y_true, y_pred)\n",
    "precision = metrics.precision_score(y_true, y_pred, pos_label = 0)\n",
    "recall = metrics.recall_score(y_true, y_pred, pos_label = 0)\n",
    "print recall\n",
    "print con\n",
    "print precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.asarray([1, 2, 3])\n",
    "b = np.asarray([1, 2, 3])\n",
    "c = []\n",
    "c.append(a)\n",
    "c.append(b)\n",
    "d = np.asarray(c)\n",
    "\n",
    "e = np.asarray([[1,2,3],[1,3,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 10\n",
    "nSpecies = 16\n",
    "for i in range(0, i) + range(i+1, nSpecies):\n",
    "    randSample = shuffle(range(0, nNegSample))\n",
    "range(0, nNegSample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17149859,  0.34299717,  0.68599434,  0.34299717,  0.51449576],\n",
       "       [ 0.17149859,  0.34299717,  0.68599434,  0.34299717,  0.51449576]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = sio.loadmat(testFile)\n",
    "# prep.normalize([[1,2,4,2,3], [1,2,4,2,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "species = np.asarray(content['dataset']['species'])[0][0][0]\n",
    "print species.shape[0]\n",
    "# species\n",
    "G1 = ['NDC1', 'NV1', 'NepCT', 'NepBH', 'NTHY', 'NDSLH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 262)\n",
      "(6,)\n",
      "18619.0\n",
      "20281.0\n",
      "20281.0\n",
      "20476.0\n",
      "20476.0\n",
      "20476.0\n",
      "(80, 262)\n",
      "20476.0\n"
     ]
    }
   ],
   "source": [
    "trainSet = np.asarray(content['dataset']['train'][0][0][0])\n",
    "valSet = np.asarray(content['dataset']['valid'])[0][0][0]\n",
    "# trainSet[0][1,:]\n",
    "print trainSet.shape\n",
    "print valSet.shape\n",
    "maxValue = 0;\n",
    "for childSet in trainSet:\n",
    "    maxValue = max(maxValue, np.amax(childSet))\n",
    "    print maxValue\n",
    "print trainSet[0].shape\n",
    "print maxValue\n",
    "# np.amax(trainSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainSet, valSet = normalize(trainSet, valSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "names = [\"Random Forest\", \"Neural Net\", \"AdaBoost\"]\n",
    "\n",
    "classifiers = [\n",
    "    RandomForestClassifier(max_depth=10, n_estimators=10, max_features=250),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confusionMat = zeros(2,2);\n",
    "for i=1:npointTest:\n",
    "    confusionMat(allLabel(i),predictLabel(i))=confusionMat(allLabel(i),predictLabel(i))+1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print len(allTrainData), len(classifiers) + 1\n",
    "for idx,(name, clf) in enumerate(zip(names, classifiers)):\n",
    "    clf.fit(allTrainData, allTrainLabels)\n",
    "    pridicLabels = clf.predict(allTestData)\n",
    "    confusionMatrix = metrics.confusion_matrix(pridicLabels, validLabels)\n",
    "#     Recall(j) = confusionMat(1,1)/(confusionMat(1,1) + confusionMat(1,2))\n",
    "#     Precision(j) = confusionMat(1,1)/(confusionMat(1,1) + confusionMat(2,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
