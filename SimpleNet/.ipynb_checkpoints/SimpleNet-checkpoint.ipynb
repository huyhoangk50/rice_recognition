{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import glob\n",
    "from scipy import sparse\n",
    "import random as rd\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "import keras\n",
    "import pandas as pd\n",
    "import csv\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# from unittest import Testcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = pd.read_csv('../data/dictionary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruct dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create species dictinary:\n",
    "    create a dictionary between species and their positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read feature from file functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readSpecFeature(file_name):\n",
    "    content = sio.loadmat(file_name)\n",
    "#     print (content)\n",
    "    return content['fullspecData']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readSpatiaFeature(file_name):\n",
    "#     print file_name\n",
    "    content = sio.loadmat(file_name)\n",
    "#     print (content)\n",
    "    return content['spatialMat']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadSpatialData(species, dictPath = '../data/dictionary.csv', dictionary = None):\n",
    "    if dictionary is None:\n",
    "        dictionary = pd.read_csv(dictPath)\n",
    "    listSpecies = dictionary['species']\n",
    "#     temp = listSpecies[listSpecies == species]\n",
    "#     print temp.index.size\n",
    "    index = listSpecies[listSpecies == species].index[0]\n",
    "    firstPath = dictionary['1st_spatial_path'][index]\n",
    "    secondPath = dictionary['2st_spatial_path'][index]\n",
    "    firstFeatures = readSpatiaFeature(firstPath)\n",
    "    secondFeatures = readSpatiaFeature(secondPath)\n",
    "    return np.concatenate((firstFeatures, secondFeatures), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare spatial sub-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:321: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:356: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "data[0] = loadSpatialData('NDC1')\n",
    "data[1] = loadSpatialData('NV1')\n",
    "data[2] = loadSpatialData('NepCoTien')\n",
    "data[3] = loadSpatialData('NepThomBacHai')\n",
    "data[4] = loadSpatialData('NepThomHungYen')\n",
    "data[5] = loadSpatialData('NepDacSanLienHoa')\n",
    "\n",
    "nClasses = data.shape[0]\n",
    "nSamples = data.shape[1]\n",
    "nFeatures = data.shape[2]\n",
    "trainRatio = 0.84\n",
    "negRatio = 0.2\n",
    "nTrainingSamples = int(trainRatio * nSamples)\n",
    "nTestSamples = nSamples - nTrainingSamples\n",
    "nNegTrainingSamples = int(negRatio * nTrainingSamples)\n",
    "\n",
    "temp = data.reshape(nSamples*nClasses,nFeatures)\n",
    "for i in range(0, nFeatures):\n",
    "    temp[:,i] = min_max_scaler.fit_transform(temp[:, i])\n",
    "print nNegTrainingSamples\n",
    "print nTrainingSamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extract sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRandomSamples(data, nSample):\n",
    "    randPosition = rd.sample(range(0, sp1Data.shape[0]), nSample)\n",
    "    samples = data[randPosition, :]\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate function:\n",
    "    separete feature set to train set and test set\n",
    "    input:\n",
    "        features: set of features\n",
    "        trainRatio: the ratio between training data and the whole data\n",
    "    return:\n",
    "        training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate(data, trainRatio = 0.84):\n",
    "    np.random.shuffle(data)\n",
    "    trainingData = data[0:int(trainRatio*data.shape[0])]\n",
    "    testingData = data[int(trainRatio*data.shape[0]): data.shape[0]]\n",
    "    return trainingData, testingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 6)\n",
      "(16, 6)\n",
      "(6, 16, 6) (6, 16, 6)\n"
     ]
    }
   ],
   "source": [
    "for i in range (0, data.shape[0]):\n",
    "    if i == 0:\n",
    "        posTraining, val = separate(data[i, :, :], trainRatio)\n",
    "        print posTraining.shape\n",
    "        negTraining, _ = separate(posTraining, negRatio)\n",
    "        print negTraining.shape\n",
    "    else:\n",
    "        posTrainingTemp, valTemp = separate(data[i, :, :], trainRatio)\n",
    "        posTraining = np.concatenate((posTraining, posTrainingTemp),axis = 0)\n",
    "        val = np.concatenate((val, valTemp), axis = 0)\n",
    "        negTrainingTemp, _ = separate(posTrainingTemp, negRatio)\n",
    "        negTraining = np.concatenate((negTraining, negTrainingTemp), axis = 0)\n",
    "posTraining = posTraining.reshape(nClasses, nTrainingSamples, nFeatures)\n",
    "val = val.reshape(nClasses, nTestSamples, nFeatures)\n",
    "negTraining = negTraining.reshape(nClasses, nNegTrainingSamples, nFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for sp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posSamples = sp1Training\n",
    "posLabels = np.asarray([0] * (posSamples.shape[0])).T\n",
    "negSamples = np.concatenate((sp2Val, sp3Val, sp4Val, sp5Val, sp6Val), axis = 0)\n",
    "negLabels = np.asarray([1] * (posSamples.shape[0])).T\n",
    "allTrainData = np.concatenate((posSamples, negSamples), axis = 0)\n",
    "allTrainLabels = np.concatenate((posLabels, negLabels), axis = 0)\n",
    "posTestSamples = sp1Test\n",
    "posTestLabels = np.asarray([0] * (posTestSamples.shape[0])).T\n",
    "negTestSamples = np.concatenate((sp2Test, sp3Test, sp4Test, sp5Test, sp6Test), axis = 0)\n",
    "negTestLabels = np.asarray([1] * (negTestSamples.shape[0])).T\n",
    "allTestData = np.concatenate((posTestSamples, negTestSamples), axis = 0)\n",
    "allTestLabels = np.concatenate((posTestLabels, negTestLabels), axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for sp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posSamples = sp2Training\n",
    "posLabels = np.asarray([0] * (posSamples.shape[0])).T\n",
    "negSamples = np.concatenate((sp1Val, sp3Val, sp4Val, sp5Val, sp6Val), axis = 0)\n",
    "negLabels = np.asarray([1] * (posSamples.shape[0])).T\n",
    "allTrainData = np.concatenate((posSamples, negSamples), axis = 0)\n",
    "allTrainLabels = np.concatenate((posLabels, negLabels), axis = 0)\n",
    "posTestSamples = sp2Test\n",
    "posTestLabels = np.asarray([0] * (posTestSamples.shape[0])).T\n",
    "negTestSamples = np.concatenate((sp1Test, sp3Test, sp4Test, sp5Test, sp6Test), axis = 0)\n",
    "negTestLabels = np.asarray([1] * (negTestSamples.shape[0])).T\n",
    "allTestData = np.concatenate((posTestSamples, negTestSamples), axis = 0)\n",
    "allTestLabels = np.concatenate((posTestLabels, negTestLabels), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for sp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posSamples = sp3Training\n",
    "posLabels = np.asarray([0] * (posSamples.shape[0])).T\n",
    "negSamples = np.concatenate((sp1Val, sp2Val, sp4Val, sp5Val, sp6Val), axis = 0)\n",
    "negLabels = np.asarray([1] * (posSamples.shape[0])).T\n",
    "allTrainData = np.concatenate((posSamples, negSamples), axis = 0)\n",
    "allTrainLabels = np.concatenate((posLabels, negLabels), axis = 0)\n",
    "posTestSamples = sp3Test\n",
    "posTestLabels = np.asarray([0] * (posTestSamples.shape[0])).T\n",
    "negTestSamples = np.concatenate((sp1Test, sp2Test, sp4Test, sp5Test, sp6Test), axis = 0)\n",
    "negTestLabels = np.asarray([1] * (negTestSamples.shape[0])).T\n",
    "allTestData = np.concatenate((posTestSamples, negTestSamples), axis = 0)\n",
    "allTestLabels = np.concatenate((posTestLabels, negTestLabels), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for sp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posSamples = sp4Training\n",
    "posLabels = np.asarray([0] * (posSamples.shape[0])).T\n",
    "negSamples = np.concatenate((sp1Val, sp2Val, sp3Val, sp5Val, sp6Val), axis = 0)\n",
    "negLabels = np.asarray([1] * (posSamples.shape[0])).T\n",
    "allTrainData = np.concatenate((posSamples, negSamples), axis = 0)\n",
    "allTrainLabels = np.concatenate((posLabels, negLabels), axis = 0)\n",
    "posTestSamples = sp4Test\n",
    "posTestLabels = np.asarray([0] * (posTestSamples.shape[0])).T\n",
    "negTestSamples = np.concatenate((sp1Test, sp2Test, sp3Test, sp5Test, sp6Test), axis = 0)\n",
    "negTestLabels = np.asarray([1] * (negTestSamples.shape[0])).T\n",
    "allTestData = np.concatenate((posTestSamples, negTestSamples), axis = 0)\n",
    "allTestLabels = np.concatenate((posTestLabels, negTestLabels), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for sp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posSamples = sp5Training\n",
    "posLabels = np.asarray([0] * (posSamples.shape[0])).T\n",
    "negSamples = np.concatenate((sp1Val, sp2Val, sp3Val, sp4Val, sp6Val), axis = 0)\n",
    "negLabels = np.asarray([1] * (posSamples.shape[0])).T\n",
    "allTrainData = np.concatenate((posSamples, negSamples), axis = 0)\n",
    "allTrainLabels = np.concatenate((posLabels, negLabels), axis = 0)\n",
    "posTestSamples = sp5Test\n",
    "posTestLabels = np.asarray([0] * (posTestSamples.shape[0])).T\n",
    "negTestSamples = np.concatenate((sp1Test, sp2Test, sp3Test, sp4Test, sp6Test), axis = 0)\n",
    "negTestLabels = np.asarray([1] * (negTestSamples.shape[0])).T\n",
    "allTestData = np.concatenate((posTestSamples, negTestSamples), axis = 0)\n",
    "allTestLabels = np.concatenate((posTestLabels, negTestLabels), axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for sp6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posSamples = sp6Training\n",
    "posLabels = np.asarray([0] * (posSamples.shape[0])).T\n",
    "negSamples = np.concatenate((sp1Val, sp2Val, sp3Val, sp4Val, sp5Val), axis = 0)\n",
    "negLabels = np.asarray([1] * (posSamples.shape[0])).T\n",
    "allTrainData = np.concatenate((posSamples, negSamples), axis = 0)\n",
    "allTrainLabels = np.concatenate((posLabels, negLabels), axis = 0)\n",
    "posTestSamples = sp6Test\n",
    "posTestLabels = np.asarray([0] * (posTestSamples.shape[0])).T\n",
    "negTestSamples = np.concatenate((sp1Test, sp2Test, sp3Test, sp4Test, sp5Test), axis = 0)\n",
    "negTestLabels = np.asarray([1] * (negTestSamples.shape[0])).T\n",
    "allTestData = np.concatenate((posTestSamples, negTestSamples), axis = 0)\n",
    "allTestLabels = np.concatenate((posTestLabels, negTestLabels), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, activation=\"relu\", kernel_initializer=\"uniform\", input_dim=6)`\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(200, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(200, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(1, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160 samples, validate on 96 samples\n",
      "Epoch 1/100\n",
      "160/160 [==============================] - 0s - loss: 0.6931 - acc: 0.5313 - val_loss: 0.6931 - val_acc: 0.4896\n",
      "Epoch 2/100\n",
      "160/160 [==============================] - 0s - loss: 0.6932 - acc: 0.4750 - val_loss: 0.6910 - val_acc: 0.7708\n",
      "Epoch 3/100\n",
      "160/160 [==============================] - 0s - loss: 0.6908 - acc: 0.5813 - val_loss: 0.6856 - val_acc: 0.6562\n",
      "Epoch 4/100\n",
      "160/160 [==============================] - 0s - loss: 0.6840 - acc: 0.5813 - val_loss: 0.6564 - val_acc: 0.6875\n",
      "Epoch 5/100\n",
      "160/160 [==============================] - 0s - loss: 0.6645 - acc: 0.6250 - val_loss: 0.6179 - val_acc: 0.6563\n",
      "Epoch 6/100\n",
      "160/160 [==============================] - 0s - loss: 0.6411 - acc: 0.6062 - val_loss: 0.6920 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "160/160 [==============================] - 0s - loss: 0.6332 - acc: 0.6125 - val_loss: 0.6845 - val_acc: 0.4896\n",
      "Epoch 8/100\n",
      "160/160 [==============================] - 0s - loss: 0.6164 - acc: 0.6438 - val_loss: 0.6741 - val_acc: 0.5104\n",
      "Epoch 9/100\n",
      "160/160 [==============================] - 0s - loss: 0.6121 - acc: 0.6438 - val_loss: 0.6466 - val_acc: 0.5833\n",
      "Epoch 10/100\n",
      "160/160 [==============================] - 0s - loss: 0.5939 - acc: 0.6375 - val_loss: 0.6422 - val_acc: 0.5833\n",
      "Epoch 11/100\n",
      "160/160 [==============================] - 0s - loss: 0.6032 - acc: 0.6438 - val_loss: 0.6731 - val_acc: 0.5521\n",
      "Epoch 12/100\n",
      "160/160 [==============================] - 0s - loss: 0.5839 - acc: 0.6938 - val_loss: 0.6492 - val_acc: 0.5938\n",
      "Epoch 13/100\n",
      "160/160 [==============================] - 0s - loss: 0.5958 - acc: 0.6188 - val_loss: 0.7458 - val_acc: 0.4688\n",
      "Epoch 14/100\n",
      "160/160 [==============================] - 0s - loss: 0.5885 - acc: 0.6438 - val_loss: 0.7962 - val_acc: 0.4063\n",
      "Epoch 15/100\n",
      "160/160 [==============================] - 0s - loss: 0.5915 - acc: 0.6563 - val_loss: 0.6336 - val_acc: 0.6042\n",
      "Epoch 16/100\n",
      "160/160 [==============================] - 0s - loss: 0.5763 - acc: 0.6688 - val_loss: 0.7236 - val_acc: 0.4688\n",
      "Epoch 17/100\n",
      "160/160 [==============================] - 0s - loss: 0.5448 - acc: 0.7250 - val_loss: 0.5752 - val_acc: 0.7917\n",
      "Epoch 18/100\n",
      "160/160 [==============================] - 0s - loss: 0.5203 - acc: 0.7750 - val_loss: 0.6825 - val_acc: 0.6250\n",
      "Epoch 19/100\n",
      "160/160 [==============================] - 0s - loss: 0.5272 - acc: 0.7188 - val_loss: 0.4952 - val_acc: 0.8750\n",
      "Epoch 20/100\n",
      "160/160 [==============================] - 0s - loss: 0.5672 - acc: 0.6750 - val_loss: 0.5733 - val_acc: 0.7708\n",
      "Epoch 21/100\n",
      "160/160 [==============================] - 0s - loss: 0.4834 - acc: 0.8062 - val_loss: 0.5704 - val_acc: 0.7500\n",
      "Epoch 22/100\n",
      "160/160 [==============================] - 0s - loss: 0.4712 - acc: 0.7875 - val_loss: 0.5483 - val_acc: 0.7708\n",
      "Epoch 23/100\n",
      "160/160 [==============================] - 0s - loss: 0.4234 - acc: 0.8187 - val_loss: 0.4432 - val_acc: 0.9167\n",
      "Epoch 24/100\n",
      "160/160 [==============================] - 0s - loss: 0.3880 - acc: 0.8562 - val_loss: 0.4313 - val_acc: 0.9271\n",
      "Epoch 25/100\n",
      "160/160 [==============================] - 0s - loss: 0.3749 - acc: 0.8500 - val_loss: 0.7633 - val_acc: 0.6146\n",
      "Epoch 26/100\n",
      "160/160 [==============================] - 0s - loss: 0.3287 - acc: 0.8687 - val_loss: 0.4036 - val_acc: 0.9167\n",
      "Epoch 27/100\n",
      "160/160 [==============================] - 0s - loss: 0.3002 - acc: 0.8875 - val_loss: 0.3389 - val_acc: 0.9479\n",
      "Epoch 28/100\n",
      "160/160 [==============================] - 0s - loss: 0.3902 - acc: 0.8125 - val_loss: 0.6438 - val_acc: 0.6875\n",
      "Epoch 29/100\n",
      "160/160 [==============================] - 0s - loss: 0.3499 - acc: 0.8062 - val_loss: 0.4175 - val_acc: 0.8750\n",
      "Epoch 30/100\n",
      "160/160 [==============================] - 0s - loss: 0.3161 - acc: 0.8625 - val_loss: 0.5786 - val_acc: 0.6875\n",
      "Epoch 31/100\n",
      "160/160 [==============================] - 0s - loss: 0.2783 - acc: 0.9062 - val_loss: 0.3708 - val_acc: 0.9375\n",
      "Epoch 32/100\n",
      "160/160 [==============================] - 0s - loss: 0.2401 - acc: 0.9125 - val_loss: 0.3305 - val_acc: 0.9375\n",
      "Epoch 33/100\n",
      "160/160 [==============================] - 0s - loss: 0.2229 - acc: 0.9250 - val_loss: 0.3585 - val_acc: 0.9271\n",
      "Epoch 34/100\n",
      "160/160 [==============================] - 0s - loss: 0.2918 - acc: 0.8812 - val_loss: 0.4300 - val_acc: 0.8750\n",
      "Epoch 35/100\n",
      "160/160 [==============================] - 0s - loss: 0.2869 - acc: 0.8937 - val_loss: 0.2964 - val_acc: 0.9583\n",
      "Epoch 36/100\n",
      "160/160 [==============================] - 0s - loss: 0.2613 - acc: 0.8875 - val_loss: 0.6886 - val_acc: 0.6667\n",
      "Epoch 37/100\n",
      "160/160 [==============================] - 0s - loss: 0.1768 - acc: 0.9437 - val_loss: 0.3179 - val_acc: 0.9375\n",
      "Epoch 38/100\n",
      "160/160 [==============================] - 0s - loss: 0.1635 - acc: 0.9625 - val_loss: 0.4896 - val_acc: 0.8125\n",
      "Epoch 39/100\n",
      "160/160 [==============================] - 0s - loss: 0.1429 - acc: 0.9750 - val_loss: 0.3192 - val_acc: 0.9271\n",
      "Epoch 40/100\n",
      "160/160 [==============================] - 0s - loss: 0.1326 - acc: 0.9562 - val_loss: 0.2489 - val_acc: 0.9896\n",
      "Epoch 41/100\n",
      "160/160 [==============================] - 0s - loss: 0.2815 - acc: 0.8687 - val_loss: 1.3033 - val_acc: 0.5938\n",
      "Epoch 42/100\n",
      "160/160 [==============================] - 0s - loss: 0.2747 - acc: 0.8812 - val_loss: 0.2819 - val_acc: 0.9375\n",
      "Epoch 43/100\n",
      "160/160 [==============================] - 0s - loss: 0.1821 - acc: 0.9375 - val_loss: 0.8168 - val_acc: 0.6667\n",
      "Epoch 44/100\n",
      "160/160 [==============================] - 0s - loss: 0.2070 - acc: 0.9187 - val_loss: 0.3507 - val_acc: 0.9167\n",
      "Epoch 45/100\n",
      "160/160 [==============================] - 0s - loss: 0.1125 - acc: 0.9750 - val_loss: 0.3365 - val_acc: 0.9167\n",
      "Epoch 46/100\n",
      "160/160 [==============================] - 0s - loss: 0.1264 - acc: 0.9687 - val_loss: 0.2499 - val_acc: 0.9687\n",
      "Epoch 47/100\n",
      "160/160 [==============================] - 0s - loss: 0.1011 - acc: 0.9812 - val_loss: 0.3855 - val_acc: 0.9167\n",
      "Epoch 48/100\n",
      "160/160 [==============================] - 0s - loss: 0.1182 - acc: 0.9562 - val_loss: 0.2460 - val_acc: 0.9687\n",
      "Epoch 49/100\n",
      "160/160 [==============================] - 0s - loss: 0.2282 - acc: 0.9125 - val_loss: 0.2442 - val_acc: 0.9583\n",
      "Epoch 50/100\n",
      "160/160 [==============================] - 0s - loss: 0.1263 - acc: 0.9500 - val_loss: 0.2592 - val_acc: 0.9479\n",
      "Epoch 51/100\n",
      "160/160 [==============================] - 0s - loss: 0.0993 - acc: 0.9625 - val_loss: 0.3197 - val_acc: 0.9375\n",
      "Epoch 52/100\n",
      "160/160 [==============================] - 0s - loss: 0.0904 - acc: 0.9750 - val_loss: 0.2861 - val_acc: 0.9375\n",
      "Epoch 53/100\n",
      "160/160 [==============================] - 0s - loss: 0.1577 - acc: 0.9500 - val_loss: 0.2606 - val_acc: 0.9479\n",
      "Epoch 54/100\n",
      "160/160 [==============================] - 0s - loss: 0.2253 - acc: 0.9187 - val_loss: 0.6293 - val_acc: 0.8125\n",
      "Epoch 55/100\n",
      "160/160 [==============================] - 0s - loss: 0.1247 - acc: 0.9437 - val_loss: 0.3412 - val_acc: 0.9062\n",
      "Epoch 56/100\n",
      "160/160 [==============================] - 0s - loss: 0.0743 - acc: 0.9875 - val_loss: 0.3011 - val_acc: 0.9479\n",
      "Epoch 57/100\n",
      "160/160 [==============================] - 0s - loss: 0.0787 - acc: 0.9687 - val_loss: 0.5404 - val_acc: 0.8229\n",
      "Epoch 58/100\n",
      "160/160 [==============================] - ETA: 0s - loss: 0.0955 - acc: 0.969 - 0s - loss: 0.0815 - acc: 0.9750 - val_loss: 0.2497 - val_acc: 0.9583\n",
      "Epoch 59/100\n",
      "160/160 [==============================] - 0s - loss: 0.0736 - acc: 0.9875 - val_loss: 0.2284 - val_acc: 0.9687\n",
      "Epoch 60/100\n",
      "160/160 [==============================] - 0s - loss: 0.1046 - acc: 0.9562 - val_loss: 0.7880 - val_acc: 0.7500\n",
      "Epoch 61/100\n",
      "160/160 [==============================] - 0s - loss: 0.1149 - acc: 0.9625 - val_loss: 0.2481 - val_acc: 0.9479\n",
      "Epoch 62/100\n",
      "160/160 [==============================] - 0s - loss: 0.1614 - acc: 0.9375 - val_loss: 0.4039 - val_acc: 0.8646\n",
      "Epoch 63/100\n",
      "160/160 [==============================] - 0s - loss: 0.2017 - acc: 0.9125 - val_loss: 0.3729 - val_acc: 0.8854\n",
      "Epoch 64/100\n",
      "160/160 [==============================] - 0s - loss: 0.0820 - acc: 0.9687 - val_loss: 0.4061 - val_acc: 0.8646\n",
      "Epoch 65/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 0s - loss: 0.0701 - acc: 0.9875 - val_loss: 0.4970 - val_acc: 0.8333\n",
      "Epoch 66/100\n",
      "160/160 [==============================] - 0s - loss: 0.0633 - acc: 0.9812 - val_loss: 0.2636 - val_acc: 0.9479\n",
      "Epoch 67/100\n",
      "160/160 [==============================] - 0s - loss: 0.0605 - acc: 0.9812 - val_loss: 0.2604 - val_acc: 0.9479\n",
      "Epoch 68/100\n",
      "160/160 [==============================] - 0s - loss: 0.0598 - acc: 0.9937 - val_loss: 0.3107 - val_acc: 0.9375\n",
      "Epoch 69/100\n",
      "160/160 [==============================] - 0s - loss: 0.0651 - acc: 0.9750 - val_loss: 0.5901 - val_acc: 0.8333\n",
      "Epoch 70/100\n",
      "160/160 [==============================] - 0s - loss: 0.1510 - acc: 0.9500 - val_loss: 0.6821 - val_acc: 0.8021\n",
      "Epoch 71/100\n",
      "160/160 [==============================] - 0s - loss: 0.1674 - acc: 0.9437 - val_loss: 0.2354 - val_acc: 0.9479\n",
      "Epoch 72/100\n",
      "160/160 [==============================] - 0s - loss: 0.1450 - acc: 0.9312 - val_loss: 0.6887 - val_acc: 0.7812\n",
      "Epoch 73/100\n",
      "160/160 [==============================] - 0s - loss: 0.1286 - acc: 0.9500 - val_loss: 0.3718 - val_acc: 0.8854\n",
      "Epoch 74/100\n",
      "160/160 [==============================] - 0s - loss: 0.0606 - acc: 0.9875 - val_loss: 0.3298 - val_acc: 0.9375\n",
      "Epoch 75/100\n",
      "160/160 [==============================] - 0s - loss: 0.0699 - acc: 0.9750 - val_loss: 0.2158 - val_acc: 0.9792\n",
      "Epoch 76/100\n",
      "160/160 [==============================] - 0s - loss: 0.1024 - acc: 0.9562 - val_loss: 0.3619 - val_acc: 0.8958\n",
      "Epoch 77/100\n",
      "160/160 [==============================] - 0s - loss: 0.1026 - acc: 0.9562 - val_loss: 0.2268 - val_acc: 0.9583\n",
      "Epoch 78/100\n",
      "160/160 [==============================] - 0s - loss: 0.0646 - acc: 0.9812 - val_loss: 0.3074 - val_acc: 0.9375\n",
      "Epoch 79/100\n",
      "160/160 [==============================] - 0s - loss: 0.0442 - acc: 0.9875 - val_loss: 0.2425 - val_acc: 0.9583\n",
      "Epoch 80/100\n",
      "160/160 [==============================] - 0s - loss: 0.0473 - acc: 0.9812 - val_loss: 0.3934 - val_acc: 0.8854\n",
      "Epoch 81/100\n",
      "160/160 [==============================] - 0s - loss: 0.0380 - acc: 0.9937 - val_loss: 0.5524 - val_acc: 0.8333\n",
      "Epoch 82/100\n",
      "160/160 [==============================] - 0s - loss: 0.0733 - acc: 0.9687 - val_loss: 0.5382 - val_acc: 0.8333\n",
      "Epoch 83/100\n",
      "160/160 [==============================] - 0s - loss: 0.0829 - acc: 0.9812 - val_loss: 0.3430 - val_acc: 0.9167\n",
      "Epoch 84/100\n",
      "160/160 [==============================] - 0s - loss: 0.0464 - acc: 0.9875 - val_loss: 0.2937 - val_acc: 0.9479\n",
      "Epoch 85/100\n",
      "160/160 [==============================] - 0s - loss: 0.0402 - acc: 0.9750 - val_loss: 0.2663 - val_acc: 0.9583\n",
      "Epoch 86/100\n",
      "160/160 [==============================] - 0s - loss: 0.0588 - acc: 0.9750 - val_loss: 0.2423 - val_acc: 0.9687\n",
      "Epoch 87/100\n",
      "160/160 [==============================] - 0s - loss: 0.1126 - acc: 0.9562 - val_loss: 0.4046 - val_acc: 0.9062\n",
      "Epoch 88/100\n",
      "160/160 [==============================] - 0s - loss: 0.0879 - acc: 0.9687 - val_loss: 0.4237 - val_acc: 0.8750\n",
      "Epoch 89/100\n",
      "160/160 [==============================] - 0s - loss: 0.0485 - acc: 0.9875 - val_loss: 0.2517 - val_acc: 0.9583\n",
      "Epoch 90/100\n",
      "160/160 [==============================] - 0s - loss: 0.0405 - acc: 0.9875 - val_loss: 0.2260 - val_acc: 0.9687\n",
      "Epoch 91/100\n",
      "160/160 [==============================] - 0s - loss: 0.0594 - acc: 0.9750 - val_loss: 0.2206 - val_acc: 0.9687\n",
      "Epoch 92/100\n",
      "160/160 [==============================] - 0s - loss: 0.0573 - acc: 0.9812 - val_loss: 0.2268 - val_acc: 0.9687\n",
      "Epoch 93/100\n",
      "160/160 [==============================] - 0s - loss: 0.0870 - acc: 0.9812 - val_loss: 0.3034 - val_acc: 0.9375\n",
      "Epoch 94/100\n",
      "160/160 [==============================] - 0s - loss: 0.0478 - acc: 0.9875 - val_loss: 0.3708 - val_acc: 0.8854\n",
      "Epoch 95/100\n",
      "160/160 [==============================] - 0s - loss: 0.0341 - acc: 0.9875 - val_loss: 0.2040 - val_acc: 0.9792\n",
      "Epoch 96/100\n",
      "160/160 [==============================] - 0s - loss: 0.0983 - acc: 0.9687 - val_loss: 0.3723 - val_acc: 0.9375\n",
      "Epoch 97/100\n",
      "160/160 [==============================] - 0s - loss: 0.1158 - acc: 0.9562 - val_loss: 0.3645 - val_acc: 0.9167\n",
      "Epoch 98/100\n",
      "160/160 [==============================] - 0s - loss: 0.0816 - acc: 0.9812 - val_loss: 0.4380 - val_acc: 0.8542\n",
      "Epoch 99/100\n",
      "160/160 [==============================] - 0s - loss: 0.0466 - acc: 0.9875 - val_loss: 0.4656 - val_acc: 0.8750\n",
      "Epoch 100/100\n",
      "160/160 [==============================] - 0s - loss: 0.0959 - acc: 0.9687 - val_loss: 0.5017 - val_acc: 0.8438\n",
      "32/96 [=========>....................] - ETA: 0s\n",
      "acc: 84.38%\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "# denseTrainingLabels = oneHostDense(trainingLabels, speciesNumber)\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=6, init='uniform', activation='relu'))\n",
    "model.add(Dense(200, init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(200, init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "# model.fit(trainingData1, trainingLabels1, epochs=200, batch_size=10, validation_data = (testData1,testLabels1))\n",
    "model.fit(allTrainData, allTrainLabels, epochs=100, batch_size=10, validation_data = (allTestData,allTestLabels), verbose = 1)\n",
    "scores = model.evaluate(allTestData,allTestLabels)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# previous experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check features dementions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 6)\n",
      "(96, 6)\n",
      "(96, 6)\n",
      "(96, 6)\n",
      "(96, 6)\n",
      "(96, 6)\n"
     ]
    }
   ],
   "source": [
    "print sp1Data.shape\n",
    "print sp2Data.shape\n",
    "print sp3Data.shape\n",
    "print sp4Data.shape\n",
    "print sp5Data.shape\n",
    "print sp6Data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test separate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3, 4, 2, 1, 7, 9, 5, 8]), array([10,  6]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separate(np.asarray([1,2,3,4,5,6,7,8,9,10]), 0.84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign labels for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seedsNumber = 96\n",
    "speciesNumber = 6\n",
    "trainingData = np.concatenate((sp1Training, sp2Training, sp3Training, sp4Training, sp5Training, sp6Training), axis = 0)\n",
    "testData = np.concatenate((sp1Test, sp2Test, sp3Test, sp4Test, sp5Test, sp6Test), axis = 0)\n",
    "testNum = seedsNumber - int(trainRatio * seedsNumber)\n",
    "# originLabels = np.asarray([])\n",
    "for i in range(0, speciesNumber):\n",
    "    if i == 0:\n",
    "        trainingLabels = np.asarray([i*1.0] * int(trainRatio * seedsNumber)).T\n",
    "        testLabels = np.asarray([i*1.0] * (seedsNumber - int(trainRatio * seedsNumber))).T\n",
    "    else:\n",
    "        trainingLabels = np.concatenate((trainingLabels, np.asarray([i*1.0] * int(trainRatio * seedsNumber)).T), axis = 0)\n",
    "        testLabels = np.concatenate((testLabels, np.asarray([i*1.0] *  testNum).T), axis = 0)\n",
    "\n",
    "# originLabels = np.asarray([0] * seedsNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.79670000e+04   2.25747859e+02   1.02253201e+02   4.52953137e-01\n",
      "   3.10111872e-02   8.91534327e-01]\n"
     ]
    }
   ],
   "source": [
    "print (trainingData[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standarVector = [100000, 1000, 100,  1, 0.1, 1]\n",
    "trainingData1 = (trainingData/standarVector)\n",
    "testData1 = (testData/standarVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainingLabels1 = keras.utils.to_categorical(trainingLabels, speciesNumber)\n",
    "testLabels1 = keras.utils.to_categorical(testLabels, speciesNumber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_shuf = range(trainingLabels.shape[0])\n",
    "shuffle(index_shuf)\n",
    "trainingLabels1= trainingLabels1[index_shuf]\n",
    "trainingData1= trainingData1[index_shuf]\n",
    "# print trainingLabels1[0:3,:]\n",
    "# print trainingData.shape\n",
    "# print testData.shape\n",
    "# print testLabels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, activation=\"relu\", kernel_initializer=\"uniform\", input_dim=6)`\n",
      "  after removing the cwd from sys.path.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(200, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \"\"\"\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(200, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(6, activation=\"sigmoid\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 480 samples, validate on 96 samples\n",
      "Epoch 1/50\n",
      "480/480 [==============================] - 0s - loss: 0.5596 - acc: 0.8264 - val_loss: 0.4614 - val_acc: 0.8333\n",
      "Epoch 2/50\n",
      "480/480 [==============================] - 0s - loss: 0.4570 - acc: 0.8333 - val_loss: 0.4510 - val_acc: 0.8333\n",
      "Epoch 3/50\n",
      "480/480 [==============================] - 0s - loss: 0.4536 - acc: 0.8333 - val_loss: 0.4509 - val_acc: 0.8333\n",
      "Epoch 4/50\n",
      "480/480 [==============================] - 0s - loss: 0.4541 - acc: 0.8333 - val_loss: 0.4509 - val_acc: 0.8333\n",
      "Epoch 5/50\n",
      "480/480 [==============================] - 0s - loss: 0.4553 - acc: 0.8333 - val_loss: 0.4506 - val_acc: 0.8333\n",
      "Epoch 6/50\n",
      "480/480 [==============================] - 0s - loss: 0.4530 - acc: 0.8333 - val_loss: 0.4504 - val_acc: 0.8333\n",
      "Epoch 7/50\n",
      "480/480 [==============================] - 0s - loss: 0.4525 - acc: 0.8333 - val_loss: 0.4499 - val_acc: 0.8333\n",
      "Epoch 8/50\n",
      "480/480 [==============================] - 0s - loss: 0.4526 - acc: 0.8333 - val_loss: 0.4496 - val_acc: 0.8333\n",
      "Epoch 9/50\n",
      "480/480 [==============================] - 0s - loss: 0.4506 - acc: 0.8333 - val_loss: 0.4484 - val_acc: 0.8333\n",
      "Epoch 10/50\n",
      "480/480 [==============================] - 0s - loss: 0.4499 - acc: 0.8333 - val_loss: 0.4474 - val_acc: 0.8333\n",
      "Epoch 11/50\n",
      "480/480 [==============================] - 0s - loss: 0.4458 - acc: 0.8333 - val_loss: 0.4461 - val_acc: 0.8333\n",
      "Epoch 12/50\n",
      "480/480 [==============================] - 0s - loss: 0.4446 - acc: 0.8333 - val_loss: 0.4435 - val_acc: 0.8333\n",
      "Epoch 13/50\n",
      "480/480 [==============================] - 0s - loss: 0.4421 - acc: 0.8333 - val_loss: 0.4353 - val_acc: 0.8333\n",
      "Epoch 14/50\n",
      "480/480 [==============================] - 0s - loss: 0.4386 - acc: 0.8330 - val_loss: 0.4348 - val_acc: 0.8333\n",
      "Epoch 15/50\n",
      "480/480 [==============================] - 0s - loss: 0.4317 - acc: 0.8326 - val_loss: 0.4290 - val_acc: 0.8333\n",
      "Epoch 16/50\n",
      "480/480 [==============================] - 0s - loss: 0.4315 - acc: 0.8309 - val_loss: 0.4322 - val_acc: 0.8333\n",
      "Epoch 17/50\n",
      "480/480 [==============================] - 0s - loss: 0.4307 - acc: 0.8330 - val_loss: 0.4256 - val_acc: 0.8333\n",
      "Epoch 18/50\n",
      "480/480 [==============================] - 0s - loss: 0.4307 - acc: 0.8340 - val_loss: 0.4300 - val_acc: 0.8316\n",
      "Epoch 19/50\n",
      "480/480 [==============================] - 0s - loss: 0.4300 - acc: 0.8333 - val_loss: 0.4227 - val_acc: 0.8333\n",
      "Epoch 20/50\n",
      "480/480 [==============================] - 0s - loss: 0.4273 - acc: 0.8344 - val_loss: 0.4222 - val_acc: 0.8333\n",
      "Epoch 21/50\n",
      "480/480 [==============================] - 0s - loss: 0.4259 - acc: 0.8333 - val_loss: 0.4230 - val_acc: 0.8333\n",
      "Epoch 22/50\n",
      "480/480 [==============================] - 0s - loss: 0.4273 - acc: 0.8333 - val_loss: 0.4207 - val_acc: 0.8333\n",
      "Epoch 23/50\n",
      "480/480 [==============================] - 0s - loss: 0.4242 - acc: 0.8333 - val_loss: 0.4226 - val_acc: 0.8351\n",
      "Epoch 24/50\n",
      "480/480 [==============================] - 0s - loss: 0.4267 - acc: 0.8326 - val_loss: 0.4226 - val_acc: 0.8333\n",
      "Epoch 25/50\n",
      "480/480 [==============================] - 0s - loss: 0.4241 - acc: 0.8337 - val_loss: 0.4236 - val_acc: 0.8333\n",
      "Epoch 26/50\n",
      "480/480 [==============================] - 0s - loss: 0.4279 - acc: 0.8326 - val_loss: 0.4194 - val_acc: 0.8333\n",
      "Epoch 27/50\n",
      "480/480 [==============================] - 0s - loss: 0.4241 - acc: 0.8340 - val_loss: 0.4274 - val_acc: 0.8316\n",
      "Epoch 28/50\n",
      "480/480 [==============================] - 0s - loss: 0.4256 - acc: 0.8354 - val_loss: 0.4190 - val_acc: 0.8333\n",
      "Epoch 29/50\n",
      "480/480 [==============================] - 0s - loss: 0.4241 - acc: 0.8330 - val_loss: 0.4174 - val_acc: 0.8333\n",
      "Epoch 30/50\n",
      "480/480 [==============================] - 0s - loss: 0.4257 - acc: 0.8354 - val_loss: 0.4215 - val_acc: 0.8333\n",
      "Epoch 31/50\n",
      "480/480 [==============================] - 0s - loss: 0.4217 - acc: 0.8337 - val_loss: 0.4185 - val_acc: 0.8351\n",
      "Epoch 32/50\n",
      "480/480 [==============================] - 0s - loss: 0.4240 - acc: 0.8326 - val_loss: 0.4180 - val_acc: 0.8333\n",
      "Epoch 33/50\n",
      "480/480 [==============================] - 0s - loss: 0.4207 - acc: 0.8354 - val_loss: 0.4159 - val_acc: 0.8351\n",
      "Epoch 34/50\n",
      "480/480 [==============================] - 0s - loss: 0.4219 - acc: 0.8337 - val_loss: 0.4186 - val_acc: 0.8333\n",
      "Epoch 35/50\n",
      "480/480 [==============================] - 0s - loss: 0.4214 - acc: 0.8333 - val_loss: 0.4143 - val_acc: 0.8333\n",
      "Epoch 36/50\n",
      "480/480 [==============================] - 0s - loss: 0.4252 - acc: 0.8333 - val_loss: 0.4149 - val_acc: 0.8351\n",
      "Epoch 37/50\n",
      "480/480 [==============================] - 0s - loss: 0.4214 - acc: 0.8344 - val_loss: 0.4143 - val_acc: 0.8351\n",
      "Epoch 38/50\n",
      "480/480 [==============================] - 0s - loss: 0.4249 - acc: 0.8333 - val_loss: 0.4184 - val_acc: 0.8333\n",
      "Epoch 39/50\n",
      "480/480 [==============================] - 0s - loss: 0.4199 - acc: 0.8358 - val_loss: 0.4154 - val_acc: 0.8351\n",
      "Epoch 40/50\n",
      "480/480 [==============================] - 0s - loss: 0.4194 - acc: 0.8340 - val_loss: 0.4141 - val_acc: 0.8333\n",
      "Epoch 41/50\n",
      "480/480 [==============================] - 0s - loss: 0.4237 - acc: 0.8316 - val_loss: 0.4168 - val_acc: 0.8351\n",
      "Epoch 42/50\n",
      "480/480 [==============================] - 0s - loss: 0.4195 - acc: 0.8347 - val_loss: 0.4171 - val_acc: 0.8333\n",
      "Epoch 43/50\n",
      "480/480 [==============================] - 0s - loss: 0.4183 - acc: 0.8326 - val_loss: 0.4137 - val_acc: 0.8333\n",
      "Epoch 44/50\n",
      "480/480 [==============================] - 0s - loss: 0.4199 - acc: 0.8333 - val_loss: 0.4138 - val_acc: 0.8333\n",
      "Epoch 45/50\n",
      "480/480 [==============================] - 0s - loss: 0.4210 - acc: 0.8326 - val_loss: 0.4149 - val_acc: 0.8333\n",
      "Epoch 46/50\n",
      "480/480 [==============================] - 0s - loss: 0.4202 - acc: 0.8337 - val_loss: 0.4150 - val_acc: 0.8333\n",
      "Epoch 47/50\n",
      "480/480 [==============================] - 0s - loss: 0.4207 - acc: 0.8330 - val_loss: 0.4120 - val_acc: 0.8351\n",
      "Epoch 48/50\n",
      "480/480 [==============================] - 0s - loss: 0.4218 - acc: 0.8333 - val_loss: 0.4114 - val_acc: 0.8368\n",
      "Epoch 49/50\n",
      "480/480 [==============================] - 0s - loss: 0.4185 - acc: 0.8344 - val_loss: 0.4126 - val_acc: 0.8351\n",
      "Epoch 50/50\n",
      "480/480 [==============================] - 0s - loss: 0.4198 - acc: 0.8312 - val_loss: 0.4131 - val_acc: 0.8351\n",
      "32/96 [=========>....................] - ETA: 0s\n",
      "acc: 83.51%\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "# denseTrainingLabels = oneHostDense(trainingLabels, speciesNumber)\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_dim=6, init='uniform', activation='relu'))\n",
    "model.add(Dense(200, init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(200, init='uniform', activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(6, init='uniform', activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "# model.fit(trainingData1, trainingLabels1, epochs=200, batch_size=10, validation_data = (testData1,testLabels1))\n",
    "model.fit(trainingData1, trainingLabels1, epochs=50, batch_size=10, validation_data = (testData1,testLabels1), verbose = 1)\n",
    "scores = model.evaluate(testData1,testLabels1)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_91 to have shape (None, 1) but got array with shape (30, 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-e6136f38d50e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestData1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLabels1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n%s: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[1;32m    890\u001b[0m                                    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                                    \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m                                    sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight)\u001b[0m\n\u001b[1;32m   1462\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1463\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1464\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1465\u001b[0m         \u001b[0;31m# Prepare inputs, delegate logic to `_test_loop`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1236\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1239\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1240\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    138\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_91 to have shape (None, 1) but got array with shape (30, 6)"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = model.evaluate(testData1[0:30,:], testLabels1[0:30,:], batch_size = 1)\n",
    "\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11476992/11490434 [============================>.] - ETA: 0sx_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 384s - loss: 0.3114 - acc: 0.9058 - val_loss: 0.0708 - val_acc: 0.9773\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 402s - loss: 0.1106 - acc: 0.9674 - val_loss: 0.0522 - val_acc: 0.9839\n",
      "Epoch 3/12\n",
      "45952/60000 [=====================>........] - ETA: 84s - loss: 0.0854 - acc: 0.9742"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-28d88ada0659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "\n",
    "def read_images_from_disk(input_queue):\n",
    "    label = input_queue[1]\n",
    "    file_contents = tf.read_file(input_queue[0])\n",
    "    example = tf.image.decode_jpeg(file_contents, channels=3)\n",
    "    # example = tf.cast( example, tf.float32 )\n",
    "    return example, label\n",
    "\n",
    "\n",
    "# weight initialization\n",
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01, name=name)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0.1, shape=shape, name=name)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# convolution\n",
    "def conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w, padding=\"VALID\", group=1):\n",
    "    '''\n",
    "    From https://github.com/ethereon/caffe-tensorflow\n",
    "    '''\n",
    "    c_i = input.get_shape()[-1]\n",
    "    assert c_i % group == 0\n",
    "    assert c_o % group == 0\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "    if group == 1:\n",
    "        conv = convolve(input, kernel)\n",
    "    else:\n",
    "        input_groups = tf.split(3, group, input)\n",
    "        kernel_groups = tf.split(3, group, kernel)\n",
    "        output_groups = [convolve(i, k) for i, k in zip(input_groups, kernel_groups)]\n",
    "        conv = tf.concat(3, output_groups)\n",
    "    return tf.reshape(tf.nn.bias_add(conv, biases), [-1] + conv.get_shape().as_list()[1:])\n",
    "\n",
    "\n",
    "def conv2d(x, W, stride_h, stride_w, padding='SAME'):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride_h, stride_w, 1], padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z1 = np.dot(W1.T, testData1) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "predicted_class = np.argmax(Z2, axis=0)\n",
    "print('training accuracy: %.2f %%' % (100*np.mean(predicted_class == testLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# temp =np.asarray([[3.79972124e+08, 3.56288960e+08, 3.82851206e+08, 4.07529309e+08 , 4.10980228e+08, 4.12379986e+08, 3.89384120e+08, 3.67045052e+08, 3.89184024e+08, 3.46503916e+08] ,[-1.81584610e+09, -1.70266680e+09, -1.82960492e+09, -1.94753893e+09 ,-1.96403051e+09, -1.97071980e+09, -1.86082502e+09, -1.75406900e+09 ,-1.85986879e+09, -1.65590512e+09] ,[3.89424382e+08, 3.65152072e+08, 3.92375085e+08, 4.17667085e+08, 4.21203850e+08, 4.22638428e+08, 3.99070513e+08, 3.76175734e+08, 3.98865440e+08, 3.55123613e+08] ,[3.72062517e+08, 3.48872349e+08, 3.74881667e+08, 3.99046063e+08, 4.02425148e+08, 4.03795767e+08, 3.81278590e+08, 3.59404538e+08, 3.81082660e+08, 3.39290993e+08], [3.77618022e+08, 3.54081587e+08, 3.80479267e+08, 4.05004478e+08, 4.08434017e+08, 4.09825103e+08, 3.86971707e+08, 3.64771039e+08, 3.86772851e+08, 3.44357166e+08], [2.96762487e+08, 2.78265671e+08, 2.99011082e+08, 3.18284957e+08, 3.20980164e+08, 3.22073390e+08, 3.04113361e+08, 2.86666299e+08, 3.03957084e+08, 2.70623441e+08]])\n",
    "# # temp =np.asarray([[3,2,1], [5,6,7]])\n",
    "# print (temp.shape)\n",
    "# print (softmax(temp[:, 1]))\n",
    "# a = softmax(temp)\n",
    "# print (a)\n",
    "# scores = [3.0e8, 1.0e8, 2e8]\n",
    "scores = [3.0, 1.0, 2]\n",
    "print(softmax(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 100 # number of points per class\n",
    "d0 = 2 # dimensionality\n",
    "C = 3 # number of classes\n",
    "X = np.zeros((d0, N*C)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*C, dtype='uint8') # class labels\n",
    "\n",
    "for j in xrange(C):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.linspace(0.0,1,N) # radius\n",
    "    t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "    X[:,ix] = np.c_[r*np.sin(t), r*np.cos(t)].T\n",
    "    y[ix] = j\n",
    "print (X[1,:])\n",
    "print ('y shape = ', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(V):\n",
    "    e_V = np.exp(V - np.max(V, axis = 0, keepdims = True))\n",
    "    Z = e_V / e_V.sum(axis = 0)\n",
    "    return Z\n",
    "\n",
    "## One-hot coding\n",
    "from scipy import sparse\n",
    "def convert_labels(y, C = 3):\n",
    "    Y = sparse.coo_matrix((np.ones_like(y),\n",
    "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
    "    return Y\n",
    "\n",
    "# cost or loss function\n",
    "def cost(Y, Yhat):\n",
    "    return -np.sum(Y*np.log(Yhat))/Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d0 = 2\n",
    "d1 = h = 100 # size of hidden layer\n",
    "d2 = C = 3\n",
    "\n",
    "# test\n",
    "\n",
    "X = trainingData1[1:3, 0:160]\n",
    "d2 = C = 2\n",
    "y = trainingLabels[0:160]\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "\n",
    "Y = convert_labels(y, C)\n",
    "N = X.shape[1]\n",
    "eta = 1 # learning rate\n",
    "for i in xrange(100):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X) + b1\n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = softmax(Z2)\n",
    "    \n",
    "    # backpropagation\n",
    "    E2 = (Yhat - Y )/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1[Z1 <= 0] = 0 # gradient of ReLU\n",
    "    dW1 = np.dot(X, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "\n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(Y, Yhat)\n",
    "        print(\"iter %d, loss: %f, \" %(i, loss))\n",
    "#         print (dW1[1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (trainingData1.shape)\n",
    "print (trainingData1[:, 7])\n",
    "print (testLabels.shape)\n",
    "# print (trainingLabels)\n",
    "# print (testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingData1 = trainingData.T\n",
    "print (trainingData1[:, 1])\n",
    "print (trainingData1[:, 90])\n",
    "# print (1/0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def softmax(V):\n",
    "#     e_V = np.exp(V - np.max(V, axis = 0, keepdims = True))\n",
    "#     Z = e_V / e_V.sum(axis = 0)\n",
    "#     return Z\n",
    "\n",
    "def oneHostDense(y, classNum = 6):\n",
    "    Y = sparse.coo_matrix((\n",
    "        np.ones_like(y), (y, np.arange(len(y)))), shape = (classNum, len(y))).toarray()\n",
    "    return Y\n",
    "\n",
    "# def cost(Y, Yhat):\n",
    "#     return -np.sum(Y*np.log(Yhat))/Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# d0 = featuresDimension \n",
    "# d1 = h = 6\n",
    "# # d1 is the output number of  hiden layer 1\n",
    "# # h is the inputnumer of input number of output layer\n",
    "# d2 = C = speciesNumber \n",
    "# # d2 is the output number of input layer (layer 2)\n",
    "# # C is the number of species\n",
    "\n",
    "# # initialize parameters randomly\n",
    "# W1 = 0.0001 * np.random.randn(d0, d1) \n",
    "# b1 = np.zeros((d1, 1))\n",
    "# #weights, bias between input layer and hidden layer\n",
    "\n",
    "# W2 = 0.0001 * np.random.randn(d1, d2) \n",
    "# b2 = np.zeros((d2, 1))\n",
    "# #weights, bias between hidden layer 2 and output layer\n",
    "\n",
    "# denseTrainingLabels = oneHostDense(trainingLabels, C)\n",
    "# trainingSampleNum = trainingData1.shape[1]\n",
    "# learningRate = 1\n",
    "\n",
    "# iteration = 10000\n",
    "\n",
    "# for i in xrange(iteration):\n",
    "#     Z1 = np.dot(W1.T, trainingData1) + b1 \n",
    "#     #Z1 is the input of first hidden layer\n",
    "#     A1 = np.maximum(Z1, 0)\n",
    "#     #A1 is the output of first hidden layer\n",
    "#     Z2 = np.dot(W2.T, A1) + b2\n",
    "#     trainingPredictions = softmax(Z2)\n",
    "#     #backpropagation\n",
    "#     E2 = (trainingPredictions - denseTrainingLabels)/ trainingSampleNum\n",
    "#     dW2 = np.dot(A1, E2.T)\n",
    "#     db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    \n",
    "#     E1 = np.dot(W2, E2)\n",
    "#     E1[Z1 <= 0] = 0 #gradient of ReLU\n",
    "#     dW1 = np.dot(trainingData1, E1.T)\n",
    "#     db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "#     #Gradient Descent update\n",
    "#     W1 += -learningRate * dW1\n",
    "#     b1 += -learningRate * db1\n",
    "#     W2 += -learningRate * dW2\n",
    "#     b2 += -learningRate * db2\n",
    "    \n",
    "#     if i%1000 == 0:\n",
    "#         loss = cost(denseTrainingLabels, trainingPredictions)\n",
    "#         print (\"iteration %d, loss: %f\" %(i, loss))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
