{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import glob\n",
    "from scipy import sparse\n",
    "import random as rd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "\n",
    "# from unittest import Testcase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruct dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create species dictinary:\n",
    "    create a dictionary between species and their positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 5)\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "data_path = '/home/hoangnh/PythonProject/RiceReconization/data/features-VIS/'\n",
    "list_spec_feature_file_path = glob.glob(data_path + '*fullricespec.mat')\n",
    "list_spatial_feature_file_path = glob.glob(data_path + '*Feat.mat')\n",
    "list_spatial_feature_file_path.sort()\n",
    "list_spec_feature_file_path.sort()\n",
    "aboveLettersNum = 67\n",
    "bellowLettersNum = 20\n",
    "\n",
    "species = list()\n",
    "firstSpecFilePath = list()\n",
    "secondSpecFilePath = list()\n",
    "\n",
    "firstSpatialFilePath = list()\n",
    "secondSpatialFilePath = list()\n",
    "for i in xrange (len(list_spec_feature_file_path)):\n",
    "    key = list_spec_feature_file_path[i][aboveLettersNum:-bellowLettersNum]\n",
    "    if i%2 ==0:\n",
    "        species.append(key)\n",
    "        firstSpatialFilePath.append(list_spatial_feature_file_path[i])\n",
    "        firstSpecFilePath.append(list_spec_feature_file_path[i])\n",
    "    else:\n",
    "        secondSpatialFilePath.append(list_spatial_feature_file_path[i])\n",
    "        secondSpecFilePath.append(list_spec_feature_file_path[i])\n",
    "# list_spec_feature_file_path.sort()\n",
    "# with open('dictionary.csv', 'w') as dictFile:\n",
    "#     fieldNames = ['species', '1st_spatial_path', '2st_spatial_path', '1st_spec_path', '2st_spec_path']\n",
    "#     writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "#     writer.writeheader()\n",
    "fieldNames = ['species', '1st_spatial_path', '2st_spatial_path', '1st_spec_path', '2st_spec_path']\n",
    "records = np.asarray([species, firstSpatialFilePath, secondSpatialFilePath, firstSpecFilePath, secondSpecFilePath]).T\n",
    "print records.shape\n",
    "with open('dictionary.csv', 'wb') as dictFile:\n",
    "#     fieldNames = ['species', '1st_spatial_path', '2st_spatial_path', '1st_spec_path', '2st_spec_path']\n",
    "    writer = csv.writer(dictFile)\n",
    "    writer.writerow(fieldNames)\n",
    "    writer.writerows(records)\n",
    "print 'end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "KN5\n",
      "/home/hoangnh/PythonProject/RiceReconization/data/features-VIS/139_KN5-01_fullricespec.mat\n",
      "/home/hoangnh/PythonProject/RiceReconization/data/features-VIS/139_KN5-01_fullricespec.mat\n",
      "/home/hoangnh/PythonProject/RiceReconization/data/features-VIS/140_KN5-02_spatialFeat.mat\n",
      "/home/hoangnh/PythonProject/RiceReconization/data/features-VIS/140_KN5-02_spatialFeat.mat\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n = np.random.randint(90)\n",
    "print n\n",
    "print species[n]\n",
    "print firstSpatialFilePath[n]\n",
    "print firstSpecFilePath[n]\n",
    "print secondSpatialFilePath[n]\n",
    "print secondSpecFilePath[n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load features and analyse them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list all mat files on datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "180\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/hoangnh/PythonProject/RiceReconization/data/features-VIS/'\n",
    "list_spec_feature_file_path = glob.glob(data_path + '*fullricespec.mat')\n",
    "list_spatial_feature_file_path = glob.glob(data_path + '*Feat.mat')\n",
    "\n",
    "print len(list_spec_feature_file_path)\n",
    "print len(list_spatial_feature_file_path)\n",
    "\n",
    "list_spatial_feature_file_path.sort()\n",
    "list_spec_feature_file_path.sort()\n",
    "\n",
    "# for filePath in list_spatial_feature_file_path:\n",
    "#     print filePath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BC15\n"
     ]
    }
   ],
   "source": [
    "# temp = glob.glob\n",
    "print list_spatial_feature_file_path[1][67: -19]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### note: khong co giong thu 67, 82, 83, 84, 105, 106"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read feature from file functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readSpecFeature(file_name):\n",
    "    content = sio.loadmat(file_name)\n",
    "    return content['fullspecData']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readSpatiaFeature(file_name):\n",
    "#     print file_name\n",
    "    content = sio.loadmat(file_name)\n",
    "    return content['spatialMat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load spatial features of a species\n",
    "#### params:\n",
    "    spPositions: postions of this species in list of species\n",
    "    seedsNum: number of seeds\n",
    "    featuresNum: number of features for one species\n",
    "    list_file_path: file paths of all species\n",
    "#### return:\n",
    "    listFeature: features of all seeds of this species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seedsNumber = 96\n",
    "featuresDimension = 6\n",
    "speciesNumber = 6\n",
    "def loadSpatialFeature(spPositions, seedsNum = 96, featuresNum = 6, list_file_path = list_spatial_feature_file_path):\n",
    "    listFeature = list()\n",
    "    for i in spPositions:\n",
    "        listFeature.append(readSpatiaFeature(list_file_path[i]))\n",
    "    listFeature = np.asarray(listFeature)\n",
    "    listFeature = listFeature.reshape(seedsNum, featuresNum)\n",
    "    return listFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare spatial sub-dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "species1Position = np.array([18,19])\n",
    "species2Position = np.array([42,43])\n",
    "species3Position = np.array([4,5])\n",
    "species4Position = np.array([26,27])\n",
    "species5Position = np.array([58,59])\n",
    "species6Position = np.array([10,11])\n",
    "sp1Features = loadSpatialFeature(species1Position)\n",
    "sp2Features = loadSpatialFeature(species2Position)\n",
    "sp3Features = loadSpatialFeature(species3Position)\n",
    "sp4Features = loadSpatialFeature(species4Position)\n",
    "sp5Features = loadSpatialFeature(species5Position)\n",
    "sp6Features = loadSpatialFeature(species6Position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check features dementions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 6)\n",
      "(96, 6)\n",
      "(96, 6)\n",
      "(96, 6)\n",
      "(96, 6)\n",
      "(96, 6)\n"
     ]
    }
   ],
   "source": [
    "print sp1Features.shape\n",
    "print sp2Features.shape\n",
    "print sp3Features.shape\n",
    "print sp4Features.shape\n",
    "print sp5Features.shape\n",
    "print sp6Features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### separate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def separate(features, trainRatio = 0.84):\n",
    "    np.random.shuffle(features)\n",
    "    trainingFeatures = features[0:int(trainRatio*features.shape[0])]\n",
    "    testFeatures = features[int(trainRatio*features.shape[0]): features.shape[0]]\n",
    "    return trainingFeatures, testFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test separate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  3,  8,  6,  2,  9,  4, 10]), array([7, 5]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separate(np.asarray([1,2,3,4,5,6,7,8,9,10]), 0.84)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainRatio = 0.84\n",
    "sp1Training, sp1Test = separate(sp1Features, trainRatio)\n",
    "sp2Training, sp2Test = separate(sp2Features, trainRatio)\n",
    "sp3Training, sp3Test = separate(sp3Features, trainRatio)\n",
    "sp4Training, sp4Test = separate(sp4Features, trainRatio)\n",
    "sp5Training, sp5Test = separate(sp5Features, trainRatio)\n",
    "sp6Training, sp6Test = separate(sp6Features, trainRatio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign labels for features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainingData = np.concatenate((sp1Training, sp2Training, sp3Training, sp4Training, sp5Training, sp6Training), axis = 0)\n",
    "testData = np.concatenate((sp1Test, sp2Test, sp3Test, sp4Test, sp5Test, sp6Test), axis = 0)\n",
    "testNum = seedsNumber - int(trainRatio * seedsNumber)\n",
    "# originLabels = np.asarray([])\n",
    "for i in range(0, speciesNumber):\n",
    "    if i == 0:\n",
    "        trainingLabels = np.asarray([i] * int(trainRatio * seedsNumber)).T\n",
    "        testLabels = np.asarray([i] * (seedsNumber - int(trainRatio * seedsNumber))).T\n",
    "    else:\n",
    "        trainingLabels = np.concatenate((trainingLabels, np.asarray([i] * int(trainRatio * seedsNumber)).T), axis = 0)\n",
    "        testLabels = np.concatenate((testLabels, np.asarray([i] *  testNum).T), axis = 0)\n",
    "\n",
    "# originLabels = np.asarray([0] * seedsNumber)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "standarVector = [100000, 1000, 100,  1, 0.1, 1]\n",
    "trainingData1 = (trainingData/standarVector).T\n",
    "testData1 = (testData/standarVector).T\n",
    "# print (trainingData1[0:1, : ]).shape\n",
    "# print (trainingData1[:, 0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### soft max, one host coding, and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(V):\n",
    "    e_V = np.exp(V - np.max(V, axis = 0, keepdims = True))\n",
    "    Z = e_V / e_V.sum(axis = 0)\n",
    "    return Z\n",
    "\n",
    "def oneHostDense(y, classNum = 6):\n",
    "    Y = sparse.coo_matrix((\n",
    "        np.ones_like(y), (y, np.arange(len(y)))), shape = (classNum, len(y))).toarray()\n",
    "    return Y\n",
    "\n",
    "def cost(Y, Yhat):\n",
    "    return -np.sum(Y*np.log(Yhat))/Y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hand made network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# d0 = featuresDimension \n",
    "# d1 = h = 6\n",
    "# # d1 is the output number of  hiden layer 1\n",
    "# # h is the inputnumer of input number of output layer\n",
    "# d2 = C = speciesNumber \n",
    "# # d2 is the output number of input layer (layer 2)\n",
    "# # C is the number of species\n",
    "\n",
    "# # initialize parameters randomly\n",
    "# W1 = 0.0001 * np.random.randn(d0, d1) \n",
    "# b1 = np.zeros((d1, 1))\n",
    "# #weights, bias between input layer and hidden layer\n",
    "\n",
    "# W2 = 0.0001 * np.random.randn(d1, d2) \n",
    "# b2 = np.zeros((d2, 1))\n",
    "# #weights, bias between hidden layer 2 and output layer\n",
    "\n",
    "# denseTrainingLabels = oneHostDense(trainingLabels, C)\n",
    "# trainingSampleNum = trainingData1.shape[1]\n",
    "# learningRate = 1\n",
    "\n",
    "# iteration = 10000\n",
    "\n",
    "# for i in xrange(iteration):\n",
    "#     Z1 = np.dot(W1.T, trainingData1) + b1 \n",
    "#     #Z1 is the input of first hidden layer\n",
    "#     A1 = np.maximum(Z1, 0)\n",
    "#     #A1 is the output of first hidden layer\n",
    "#     Z2 = np.dot(W2.T, A1) + b2\n",
    "#     trainingPredictions = softmax(Z2)\n",
    "#     #backpropagation\n",
    "#     E2 = (trainingPredictions - denseTrainingLabels)/ trainingSampleNum\n",
    "#     dW2 = np.dot(A1, E2.T)\n",
    "#     db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    \n",
    "#     E1 = np.dot(W2, E2)\n",
    "#     E1[Z1 <= 0] = 0 #gradient of ReLU\n",
    "#     dW1 = np.dot(trainingData1, E1.T)\n",
    "#     db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "    \n",
    "#     #Gradient Descent update\n",
    "#     W1 += -learningRate * dW1\n",
    "#     b1 += -learningRate * db1\n",
    "#     W2 += -learningRate * dW2\n",
    "#     b2 += -learningRate * db2\n",
    "    \n",
    "#     if i%1000 == 0:\n",
    "#         loss = cost(denseTrainingLabels, trainingPredictions)\n",
    "#         print (\"iteration %d, loss: %f\" %(i, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "480/480 [==============================] - 0s - loss: 0.4512 - acc: 0.8333     \n",
      "Epoch 2/200\n",
      "480/480 [==============================] - 0s - loss: 0.4508 - acc: 0.8333     \n",
      "Epoch 3/200\n",
      "480/480 [==============================] - 0s - loss: 0.4506 - acc: 0.8333     \n",
      "Epoch 4/200\n",
      "480/480 [==============================] - 0s - loss: 0.4504 - acc: 0.8333     \n",
      "Epoch 5/200\n",
      "480/480 [==============================] - 0s - loss: 0.4503 - acc: 0.8333     \n",
      "Epoch 6/200\n",
      "480/480 [==============================] - 0s - loss: 0.4502 - acc: 0.8333     \n",
      "Epoch 7/200\n",
      "480/480 [==============================] - 0s - loss: 0.4499 - acc: 0.8333     \n",
      "Epoch 8/200\n",
      "480/480 [==============================] - 0s - loss: 0.4498 - acc: 0.8333     \n",
      "Epoch 9/200\n",
      "480/480 [==============================] - 0s - loss: 0.4495 - acc: 0.8333     \n",
      "Epoch 10/200\n",
      "480/480 [==============================] - 0s - loss: 0.4492 - acc: 0.8333     \n",
      "Epoch 11/200\n",
      "480/480 [==============================] - 0s - loss: 0.4487 - acc: 0.8333     \n",
      "Epoch 12/200\n",
      "480/480 [==============================] - 0s - loss: 0.4476 - acc: 0.8333     \n",
      "Epoch 13/200\n",
      "480/480 [==============================] - 0s - loss: 0.4464 - acc: 0.8333     \n",
      "Epoch 14/200\n",
      "480/480 [==============================] - 0s - loss: 0.4453 - acc: 0.8333     \n",
      "Epoch 15/200\n",
      "480/480 [==============================] - 0s - loss: 0.4438 - acc: 0.8333     \n",
      "Epoch 16/200\n",
      "480/480 [==============================] - 0s - loss: 0.4425 - acc: 0.8333     \n",
      "Epoch 17/200\n",
      "480/480 [==============================] - 0s - loss: 0.4409 - acc: 0.8333     \n",
      "Epoch 18/200\n",
      "480/480 [==============================] - 0s - loss: 0.4392 - acc: 0.8333     \n",
      "Epoch 19/200\n",
      "480/480 [==============================] - 0s - loss: 0.4374 - acc: 0.8333     \n",
      "Epoch 20/200\n",
      "480/480 [==============================] - 0s - loss: 0.4353 - acc: 0.8333     \n",
      "Epoch 21/200\n",
      "480/480 [==============================] - 0s - loss: 0.4334 - acc: 0.8333     \n",
      "Epoch 22/200\n",
      "480/480 [==============================] - 0s - loss: 0.4311 - acc: 0.8333     \n",
      "Epoch 23/200\n",
      "480/480 [==============================] - 0s - loss: 0.4294 - acc: 0.8333     \n",
      "Epoch 24/200\n",
      "480/480 [==============================] - 0s - loss: 0.4270 - acc: 0.8333     \n",
      "Epoch 25/200\n",
      "480/480 [==============================] - 0s - loss: 0.4247 - acc: 0.8333     \n",
      "Epoch 26/200\n",
      "480/480 [==============================] - 0s - loss: 0.4226 - acc: 0.8333     \n",
      "Epoch 27/200\n",
      "480/480 [==============================] - 0s - loss: 0.4206 - acc: 0.8333     \n",
      "Epoch 28/200\n",
      "480/480 [==============================] - 0s - loss: 0.4180 - acc: 0.8333     \n",
      "Epoch 29/200\n",
      "480/480 [==============================] - 0s - loss: 0.4160 - acc: 0.8333     \n",
      "Epoch 30/200\n",
      "480/480 [==============================] - 0s - loss: 0.4137 - acc: 0.8333     \n",
      "Epoch 31/200\n",
      "480/480 [==============================] - 0s - loss: 0.4116 - acc: 0.8333     \n",
      "Epoch 32/200\n",
      "480/480 [==============================] - 0s - loss: 0.4092 - acc: 0.8333     \n",
      "Epoch 33/200\n",
      "480/480 [==============================] - 0s - loss: 0.4072 - acc: 0.8333     \n",
      "Epoch 34/200\n",
      "480/480 [==============================] - 0s - loss: 0.4048 - acc: 0.8333     \n",
      "Epoch 35/200\n",
      "480/480 [==============================] - 0s - loss: 0.4027 - acc: 0.8333     \n",
      "Epoch 36/200\n",
      "480/480 [==============================] - 0s - loss: 0.4006 - acc: 0.8333     \n",
      "Epoch 37/200\n",
      "480/480 [==============================] - 0s - loss: 0.3973 - acc: 0.8333     \n",
      "Epoch 38/200\n",
      "480/480 [==============================] - 0s - loss: 0.3947 - acc: 0.8333     \n",
      "Epoch 39/200\n",
      "480/480 [==============================] - 0s - loss: 0.3912 - acc: 0.8333     \n",
      "Epoch 40/200\n",
      "480/480 [==============================] - 0s - loss: 0.3880 - acc: 0.8333     \n",
      "Epoch 41/200\n",
      "480/480 [==============================] - 0s - loss: 0.3860 - acc: 0.8333     \n",
      "Epoch 42/200\n",
      "480/480 [==============================] - 0s - loss: 0.3823 - acc: 0.8333     \n",
      "Epoch 43/200\n",
      "480/480 [==============================] - 0s - loss: 0.3803 - acc: 0.8333     \n",
      "Epoch 44/200\n",
      "480/480 [==============================] - 0s - loss: 0.3766 - acc: 0.8333     \n",
      "Epoch 45/200\n",
      "480/480 [==============================] - 0s - loss: 0.3744 - acc: 0.8333     \n",
      "Epoch 46/200\n",
      "480/480 [==============================] - 0s - loss: 0.3721 - acc: 0.8333     \n",
      "Epoch 47/200\n",
      "480/480 [==============================] - 0s - loss: 0.3688 - acc: 0.8333     \n",
      "Epoch 48/200\n",
      "480/480 [==============================] - 0s - loss: 0.3660 - acc: 0.8333     \n",
      "Epoch 49/200\n",
      "480/480 [==============================] - 0s - loss: 0.3637 - acc: 0.8333     \n",
      "Epoch 50/200\n",
      "480/480 [==============================] - 0s - loss: 0.3619 - acc: 0.8333     \n",
      "Epoch 51/200\n",
      "480/480 [==============================] - 0s - loss: 0.3596 - acc: 0.8333     \n",
      "Epoch 52/200\n",
      "480/480 [==============================] - 0s - loss: 0.3568 - acc: 0.8333     \n",
      "Epoch 53/200\n",
      "480/480 [==============================] - 0s - loss: 0.3542 - acc: 0.8333     \n",
      "Epoch 54/200\n",
      "480/480 [==============================] - 0s - loss: 0.3518 - acc: 0.8333     \n",
      "Epoch 55/200\n",
      "480/480 [==============================] - 0s - loss: 0.3504 - acc: 0.8333     \n",
      "Epoch 56/200\n",
      "480/480 [==============================] - 0s - loss: 0.3480 - acc: 0.8333     \n",
      "Epoch 57/200\n",
      "480/480 [==============================] - 0s - loss: 0.3457 - acc: 0.8333     \n",
      "Epoch 58/200\n",
      "480/480 [==============================] - 0s - loss: 0.3430 - acc: 0.8333     \n",
      "Epoch 59/200\n",
      "480/480 [==============================] - 0s - loss: 0.3421 - acc: 0.8333     \n",
      "Epoch 60/200\n",
      "480/480 [==============================] - 0s - loss: 0.3389 - acc: 0.8333     \n",
      "Epoch 61/200\n",
      "480/480 [==============================] - 0s - loss: 0.3366 - acc: 0.8333     \n",
      "Epoch 62/200\n",
      "480/480 [==============================] - 0s - loss: 0.3349 - acc: 0.8333     \n",
      "Epoch 63/200\n",
      "480/480 [==============================] - 0s - loss: 0.3328 - acc: 0.8333     \n",
      "Epoch 64/200\n",
      "480/480 [==============================] - 0s - loss: 0.3307 - acc: 0.8333     \n",
      "Epoch 65/200\n",
      "480/480 [==============================] - 0s - loss: 0.3287 - acc: 0.8333     \n",
      "Epoch 66/200\n",
      "480/480 [==============================] - 0s - loss: 0.3267 - acc: 0.8333     \n",
      "Epoch 67/200\n",
      "480/480 [==============================] - 0s - loss: 0.3249 - acc: 0.8458     \n",
      "Epoch 68/200\n",
      "480/480 [==============================] - 0s - loss: 0.3226 - acc: 0.8507     \n",
      "Epoch 69/200\n",
      "480/480 [==============================] - 0s - loss: 0.3203 - acc: 0.8497     - ETA: 0s - loss: 0.3262 - acc: 0.84\n",
      "Epoch 70/200\n",
      "480/480 [==============================] - 0s - loss: 0.3177 - acc: 0.8510     \n",
      "Epoch 71/200\n",
      "480/480 [==============================] - 0s - loss: 0.3161 - acc: 0.8500     \n",
      "Epoch 72/200\n",
      "480/480 [==============================] - 0s - loss: 0.3142 - acc: 0.8528     \n",
      "Epoch 73/200\n",
      "480/480 [==============================] - 0s - loss: 0.3121 - acc: 0.8507     \n",
      "Epoch 74/200\n",
      "480/480 [==============================] - 0s - loss: 0.3102 - acc: 0.8517     \n",
      "Epoch 75/200\n",
      "480/480 [==============================] - 0s - loss: 0.3080 - acc: 0.8531     \n",
      "Epoch 76/200\n",
      "480/480 [==============================] - 0s - loss: 0.3053 - acc: 0.8531     \n",
      "Epoch 77/200\n",
      "480/480 [==============================] - 0s - loss: 0.3036 - acc: 0.8524     \n",
      "Epoch 78/200\n",
      "480/480 [==============================] - 0s - loss: 0.3017 - acc: 0.8528     \n",
      "Epoch 79/200\n",
      "480/480 [==============================] - 0s - loss: 0.2996 - acc: 0.8542     \n",
      "Epoch 80/200\n",
      "480/480 [==============================] - 0s - loss: 0.2984 - acc: 0.8538     \n",
      "Epoch 81/200\n",
      "480/480 [==============================] - 0s - loss: 0.2957 - acc: 0.8545     \n",
      "Epoch 82/200\n",
      "480/480 [==============================] - 0s - loss: 0.2940 - acc: 0.8542     \n",
      "Epoch 83/200\n",
      "480/480 [==============================] - 0s - loss: 0.2914 - acc: 0.8576     \n",
      "Epoch 84/200\n",
      "480/480 [==============================] - 0s - loss: 0.2899 - acc: 0.8635     \n",
      "Epoch 85/200\n",
      "480/480 [==============================] - 0s - loss: 0.2879 - acc: 0.8705     \n",
      "Epoch 86/200\n",
      "480/480 [==============================] - 0s - loss: 0.2863 - acc: 0.8722     \n",
      "Epoch 87/200\n",
      "480/480 [==============================] - 0s - loss: 0.2840 - acc: 0.8743     \n",
      "Epoch 88/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/480 [==============================] - 0s - loss: 0.2828 - acc: 0.8740     \n",
      "Epoch 89/200\n",
      "480/480 [==============================] - 0s - loss: 0.2801 - acc: 0.8757     \n",
      "Epoch 90/200\n",
      "480/480 [==============================] - 0s - loss: 0.2799 - acc: 0.8753     \n",
      "Epoch 91/200\n",
      "480/480 [==============================] - 0s - loss: 0.2771 - acc: 0.8771     \n",
      "Epoch 92/200\n",
      "480/480 [==============================] - 0s - loss: 0.2749 - acc: 0.8764     \n",
      "Epoch 93/200\n",
      "480/480 [==============================] - 0s - loss: 0.2731 - acc: 0.8771     \n",
      "Epoch 94/200\n",
      "480/480 [==============================] - 0s - loss: 0.2711 - acc: 0.8792     \n",
      "Epoch 95/200\n",
      "480/480 [==============================] - 0s - loss: 0.2697 - acc: 0.8771     \n",
      "Epoch 96/200\n",
      "480/480 [==============================] - 0s - loss: 0.2683 - acc: 0.8781     \n",
      "Epoch 97/200\n",
      "480/480 [==============================] - 0s - loss: 0.2667 - acc: 0.8781     \n",
      "Epoch 98/200\n",
      "480/480 [==============================] - 0s - loss: 0.2664 - acc: 0.8774     \n",
      "Epoch 99/200\n",
      "480/480 [==============================] - 0s - loss: 0.2639 - acc: 0.8785     \n",
      "Epoch 100/200\n",
      "480/480 [==============================] - 0s - loss: 0.2622 - acc: 0.8781     \n",
      "Epoch 101/200\n",
      "480/480 [==============================] - 0s - loss: 0.2612 - acc: 0.8778     \n",
      "Epoch 102/200\n",
      "480/480 [==============================] - 0s - loss: 0.2590 - acc: 0.8778     \n",
      "Epoch 103/200\n",
      "480/480 [==============================] - 0s - loss: 0.2575 - acc: 0.8785     \n",
      "Epoch 104/200\n",
      "480/480 [==============================] - 0s - loss: 0.2567 - acc: 0.8781     \n",
      "Epoch 105/200\n",
      "480/480 [==============================] - 0s - loss: 0.2549 - acc: 0.8788     \n",
      "Epoch 106/200\n",
      "480/480 [==============================] - 0s - loss: 0.2536 - acc: 0.8788     \n",
      "Epoch 107/200\n",
      "480/480 [==============================] - 0s - loss: 0.2521 - acc: 0.8799     \n",
      "Epoch 108/200\n",
      "480/480 [==============================] - 0s - loss: 0.2507 - acc: 0.8788     \n",
      "Epoch 109/200\n",
      "480/480 [==============================] - 0s - loss: 0.2504 - acc: 0.8812     \n",
      "Epoch 110/200\n",
      "480/480 [==============================] - 0s - loss: 0.2481 - acc: 0.8809     \n",
      "Epoch 111/200\n",
      "480/480 [==============================] - 0s - loss: 0.2469 - acc: 0.8819     \n",
      "Epoch 112/200\n",
      "480/480 [==============================] - 0s - loss: 0.2453 - acc: 0.8816     \n",
      "Epoch 113/200\n",
      "480/480 [==============================] - 0s - loss: 0.2441 - acc: 0.8844     \n",
      "Epoch 114/200\n",
      "480/480 [==============================] - 0s - loss: 0.2429 - acc: 0.8844     \n",
      "Epoch 115/200\n",
      "480/480 [==============================] - 0s - loss: 0.2426 - acc: 0.8872     \n",
      "Epoch 116/200\n",
      "480/480 [==============================] - 0s - loss: 0.2411 - acc: 0.8875     \n",
      "Epoch 117/200\n",
      "480/480 [==============================] - 0s - loss: 0.2396 - acc: 0.8868     \n",
      "Epoch 118/200\n",
      "480/480 [==============================] - 0s - loss: 0.2378 - acc: 0.8903     \n",
      "Epoch 119/200\n",
      "480/480 [==============================] - 0s - loss: 0.2377 - acc: 0.8878     \n",
      "Epoch 120/200\n",
      "480/480 [==============================] - 0s - loss: 0.2359 - acc: 0.8899     \n",
      "Epoch 121/200\n",
      "480/480 [==============================] - 0s - loss: 0.2350 - acc: 0.8903     \n",
      "Epoch 122/200\n",
      "480/480 [==============================] - 0s - loss: 0.2337 - acc: 0.8899     \n",
      "Epoch 123/200\n",
      "480/480 [==============================] - 0s - loss: 0.2329 - acc: 0.8906     \n",
      "Epoch 124/200\n",
      "480/480 [==============================] - 0s - loss: 0.2322 - acc: 0.8913     \n",
      "Epoch 125/200\n",
      "480/480 [==============================] - 0s - loss: 0.2307 - acc: 0.8906     \n",
      "Epoch 126/200\n",
      "480/480 [==============================] - 0s - loss: 0.2295 - acc: 0.8910     \n",
      "Epoch 127/200\n",
      "480/480 [==============================] - 0s - loss: 0.2283 - acc: 0.8937     \n",
      "Epoch 128/200\n",
      "480/480 [==============================] - 0s - loss: 0.2278 - acc: 0.8917     \n",
      "Epoch 129/200\n",
      "480/480 [==============================] - 0s - loss: 0.2274 - acc: 0.8903     \n",
      "Epoch 130/200\n",
      "480/480 [==============================] - 0s - loss: 0.2258 - acc: 0.8931     \n",
      "Epoch 131/200\n",
      "480/480 [==============================] - 0s - loss: 0.2247 - acc: 0.8917     \n",
      "Epoch 132/200\n",
      "480/480 [==============================] - 0s - loss: 0.2240 - acc: 0.8934     \n",
      "Epoch 133/200\n",
      "480/480 [==============================] - 0s - loss: 0.2235 - acc: 0.8944     \n",
      "Epoch 134/200\n",
      "480/480 [==============================] - 0s - loss: 0.2232 - acc: 0.8931     \n",
      "Epoch 135/200\n",
      "480/480 [==============================] - 0s - loss: 0.2220 - acc: 0.8941     \n",
      "Epoch 136/200\n",
      "480/480 [==============================] - 0s - loss: 0.2206 - acc: 0.8948     \n",
      "Epoch 137/200\n",
      "480/480 [==============================] - 0s - loss: 0.2195 - acc: 0.8955     \n",
      "Epoch 138/200\n",
      "480/480 [==============================] - 0s - loss: 0.2200 - acc: 0.8941     \n",
      "Epoch 139/200\n",
      "480/480 [==============================] - 0s - loss: 0.2182 - acc: 0.8951     \n",
      "Epoch 140/200\n",
      "480/480 [==============================] - 0s - loss: 0.2177 - acc: 0.8955     \n",
      "Epoch 141/200\n",
      "480/480 [==============================] - 0s - loss: 0.2173 - acc: 0.8965     \n",
      "Epoch 142/200\n",
      "480/480 [==============================] - 0s - loss: 0.2164 - acc: 0.8941     \n",
      "Epoch 143/200\n",
      "480/480 [==============================] - 0s - loss: 0.2158 - acc: 0.8958     \n",
      "Epoch 144/200\n",
      "480/480 [==============================] - 0s - loss: 0.2151 - acc: 0.8976     \n",
      "Epoch 145/200\n",
      "480/480 [==============================] - 0s - loss: 0.2141 - acc: 0.8972     \n",
      "Epoch 146/200\n",
      "480/480 [==============================] - 0s - loss: 0.2131 - acc: 0.8976     \n",
      "Epoch 147/200\n",
      "480/480 [==============================] - 0s - loss: 0.2149 - acc: 0.8958     \n",
      "Epoch 148/200\n",
      "480/480 [==============================] - 0s - loss: 0.2120 - acc: 0.8948     \n",
      "Epoch 149/200\n",
      "480/480 [==============================] - 0s - loss: 0.2116 - acc: 0.8986     \n",
      "Epoch 150/200\n",
      "480/480 [==============================] - 0s - loss: 0.2120 - acc: 0.8944     \n",
      "Epoch 151/200\n",
      "480/480 [==============================] - 0s - loss: 0.2110 - acc: 0.8958     \n",
      "Epoch 152/200\n",
      "480/480 [==============================] - 0s - loss: 0.2102 - acc: 0.8979     \n",
      "Epoch 153/200\n",
      "480/480 [==============================] - 0s - loss: 0.2112 - acc: 0.8962     \n",
      "Epoch 154/200\n",
      "480/480 [==============================] - 0s - loss: 0.2100 - acc: 0.8965     \n",
      "Epoch 155/200\n",
      "480/480 [==============================] - 0s - loss: 0.2091 - acc: 0.8986     \n",
      "Epoch 156/200\n",
      "480/480 [==============================] - 0s - loss: 0.2093 - acc: 0.8965     \n",
      "Epoch 157/200\n",
      "480/480 [==============================] - 0s - loss: 0.2083 - acc: 0.8969     \n",
      "Epoch 158/200\n",
      "480/480 [==============================] - 0s - loss: 0.2081 - acc: 0.8972     \n",
      "Epoch 159/200\n",
      "480/480 [==============================] - 0s - loss: 0.2074 - acc: 0.8972     \n",
      "Epoch 160/200\n",
      "480/480 [==============================] - 0s - loss: 0.2067 - acc: 0.8979     \n",
      "Epoch 161/200\n",
      "480/480 [==============================] - 0s - loss: 0.2072 - acc: 0.8969     \n",
      "Epoch 162/200\n",
      "480/480 [==============================] - 0s - loss: 0.2068 - acc: 0.8976     \n",
      "Epoch 163/200\n",
      "480/480 [==============================] - 0s - loss: 0.2069 - acc: 0.8979     \n",
      "Epoch 164/200\n",
      "480/480 [==============================] - 0s - loss: 0.2071 - acc: 0.8972     \n",
      "Epoch 165/200\n",
      "480/480 [==============================] - 0s - loss: 0.2054 - acc: 0.8979     \n",
      "Epoch 166/200\n",
      "480/480 [==============================] - 0s - loss: 0.2062 - acc: 0.8976     \n",
      "Epoch 167/200\n",
      "480/480 [==============================] - 0s - loss: 0.2048 - acc: 0.8969     \n",
      "Epoch 168/200\n",
      "480/480 [==============================] - 0s - loss: 0.2058 - acc: 0.8962     \n",
      "Epoch 169/200\n",
      "480/480 [==============================] - 0s - loss: 0.2037 - acc: 0.9003     \n",
      "Epoch 170/200\n",
      "480/480 [==============================] - 0s - loss: 0.2044 - acc: 0.8976     \n",
      "Epoch 171/200\n",
      "480/480 [==============================] - 0s - loss: 0.2035 - acc: 0.8983     \n",
      "Epoch 172/200\n",
      "480/480 [==============================] - 0s - loss: 0.2044 - acc: 0.8976     \n",
      "Epoch 173/200\n",
      "480/480 [==============================] - 0s - loss: 0.2028 - acc: 0.8983     \n",
      "Epoch 174/200\n",
      "480/480 [==============================] - 0s - loss: 0.2032 - acc: 0.8979     \n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480/480 [==============================] - 0s - loss: 0.2032 - acc: 0.8972     \n",
      "Epoch 176/200\n",
      "480/480 [==============================] - 0s - loss: 0.2025 - acc: 0.8993     \n",
      "Epoch 177/200\n",
      "480/480 [==============================] - 0s - loss: 0.2025 - acc: 0.8972     \n",
      "Epoch 178/200\n",
      "480/480 [==============================] - 0s - loss: 0.2024 - acc: 0.8990     \n",
      "Epoch 179/200\n",
      "480/480 [==============================] - 0s - loss: 0.2031 - acc: 0.8965     \n",
      "Epoch 180/200\n",
      "480/480 [==============================] - 0s - loss: 0.2023 - acc: 0.8990     \n",
      "Epoch 181/200\n",
      "480/480 [==============================] - 0s - loss: 0.2016 - acc: 0.8993     \n",
      "Epoch 182/200\n",
      "480/480 [==============================] - 0s - loss: 0.2014 - acc: 0.8990     \n",
      "Epoch 183/200\n",
      "480/480 [==============================] - 0s - loss: 0.2001 - acc: 0.8990     \n",
      "Epoch 184/200\n",
      "480/480 [==============================] - 0s - loss: 0.2012 - acc: 0.9000     \n",
      "Epoch 185/200\n",
      "480/480 [==============================] - 0s - loss: 0.2014 - acc: 0.8979     \n",
      "Epoch 186/200\n",
      "480/480 [==============================] - 0s - loss: 0.2004 - acc: 0.8990     \n",
      "Epoch 187/200\n",
      "480/480 [==============================] - 0s - loss: 0.1998 - acc: 0.9000     \n",
      "Epoch 188/200\n",
      "480/480 [==============================] - 0s - loss: 0.1999 - acc: 0.8986     \n",
      "Epoch 189/200\n",
      "480/480 [==============================] - 0s - loss: 0.2007 - acc: 0.8983     \n",
      "Epoch 190/200\n",
      "480/480 [==============================] - 0s - loss: 0.1995 - acc: 0.8986     \n",
      "Epoch 191/200\n",
      "480/480 [==============================] - 0s - loss: 0.1992 - acc: 0.8990     \n",
      "Epoch 192/200\n",
      "480/480 [==============================] - 0s - loss: 0.1996 - acc: 0.8972     \n",
      "Epoch 193/200\n",
      "480/480 [==============================] - 0s - loss: 0.1998 - acc: 0.8983     \n",
      "Epoch 194/200\n",
      "480/480 [==============================] - 0s - loss: 0.1992 - acc: 0.8983     \n",
      "Epoch 195/200\n",
      "480/480 [==============================] - 0s - loss: 0.1987 - acc: 0.9000     \n",
      "Epoch 196/200\n",
      "480/480 [==============================] - 0s - loss: 0.2002 - acc: 0.8983     \n",
      "Epoch 197/200\n",
      "480/480 [==============================] - 0s - loss: 0.1992 - acc: 0.8983     \n",
      "Epoch 198/200\n",
      "480/480 [==============================] - 0s - loss: 0.1980 - acc: 0.8993     \n",
      "Epoch 199/200\n",
      "480/480 [==============================] - 0s - loss: 0.2002 - acc: 0.8983     \n",
      "Epoch 200/200\n",
      "480/480 [==============================] - 0s - loss: 0.1978 - acc: 0.9007     \n",
      "32/96 [=========>....................] - ETA: 0s\n",
      "acc: 89.24%\n"
     ]
    }
   ],
   "source": [
    "# create model\n",
    "denseTrainingLabels = oneHostDense(trainingLabels, speciesNumber)\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=6, activation='relu'))\n",
    "# model.add(Flatten(name='flatten'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(trainingData1.T, denseTrainingLabels.T, epochs=200, batch_size=10)\n",
    "scores = model.evaluate(testData1.T, oneHostDense(testLabels).T)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = np.dot(W1.T, testData1) + b1\n",
    "A1 = np.maximum(Z1, 0)\n",
    "Z2 = np.dot(W2.T, A1) + b2\n",
    "predicted_class = np.argmax(Z2, axis=0)\n",
    "print('training accuracy: %.2f %%' % (100*np.mean(predicted_class == testLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp =np.asarray([[3.79972124e+08, 3.56288960e+08, 3.82851206e+08, 4.07529309e+08 , 4.10980228e+08, 4.12379986e+08, 3.89384120e+08, 3.67045052e+08, 3.89184024e+08, 3.46503916e+08] ,[-1.81584610e+09, -1.70266680e+09, -1.82960492e+09, -1.94753893e+09 ,-1.96403051e+09, -1.97071980e+09, -1.86082502e+09, -1.75406900e+09 ,-1.85986879e+09, -1.65590512e+09] ,[3.89424382e+08, 3.65152072e+08, 3.92375085e+08, 4.17667085e+08, 4.21203850e+08, 4.22638428e+08, 3.99070513e+08, 3.76175734e+08, 3.98865440e+08, 3.55123613e+08] ,[3.72062517e+08, 3.48872349e+08, 3.74881667e+08, 3.99046063e+08, 4.02425148e+08, 4.03795767e+08, 3.81278590e+08, 3.59404538e+08, 3.81082660e+08, 3.39290993e+08], [3.77618022e+08, 3.54081587e+08, 3.80479267e+08, 4.05004478e+08, 4.08434017e+08, 4.09825103e+08, 3.86971707e+08, 3.64771039e+08, 3.86772851e+08, 3.44357166e+08], [2.96762487e+08, 2.78265671e+08, 2.99011082e+08, 3.18284957e+08, 3.20980164e+08, 3.22073390e+08, 3.04113361e+08, 2.86666299e+08, 3.03957084e+08, 2.70623441e+08]])\n",
    "# # temp =np.asarray([[3,2,1], [5,6,7]])\n",
    "# print (temp.shape)\n",
    "# print (softmax(temp[:, 1]))\n",
    "# a = softmax(temp)\n",
    "# print (a)\n",
    "# scores = [3.0e8, 1.0e8, 2e8]\n",
    "scores = [3.0, 1.0, 2]\n",
    "print(softmax(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 100 # number of points per class\n",
    "d0 = 2 # dimensionality\n",
    "C = 3 # number of classes\n",
    "X = np.zeros((d0, N*C)) # data matrix (each row = single example)\n",
    "y = np.zeros(N*C, dtype='uint8') # class labels\n",
    "\n",
    "for j in xrange(C):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.linspace(0.0,1,N) # radius\n",
    "    t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "    X[:,ix] = np.c_[r*np.sin(t), r*np.cos(t)].T\n",
    "    y[ix] = j\n",
    "print (X[1,:])\n",
    "print ('y shape = ', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(V):\n",
    "    e_V = np.exp(V - np.max(V, axis = 0, keepdims = True))\n",
    "    Z = e_V / e_V.sum(axis = 0)\n",
    "    return Z\n",
    "\n",
    "## One-hot coding\n",
    "from scipy import sparse\n",
    "def convert_labels(y, C = 3):\n",
    "    Y = sparse.coo_matrix((np.ones_like(y),\n",
    "        (y, np.arange(len(y)))), shape = (C, len(y))).toarray()\n",
    "    return Y\n",
    "\n",
    "# cost or loss function\n",
    "def cost(Y, Yhat):\n",
    "    return -np.sum(Y*np.log(Yhat))/Y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = 2\n",
    "d1 = h = 100 # size of hidden layer\n",
    "d2 = C = 3\n",
    "\n",
    "# test\n",
    "\n",
    "X = trainingData1[1:3, 0:160]\n",
    "d2 = C = 2\n",
    "y = trainingLabels[0:160]\n",
    "# initialize parameters randomly\n",
    "W1 = 0.01*np.random.randn(d0, d1)\n",
    "b1 = np.zeros((d1, 1))\n",
    "W2 = 0.01*np.random.randn(d1, d2)\n",
    "b2 = np.zeros((d2, 1))\n",
    "\n",
    "\n",
    "Y = convert_labels(y, C)\n",
    "N = X.shape[1]\n",
    "eta = 1 # learning rate\n",
    "for i in xrange(100):\n",
    "    ## Feedforward\n",
    "    Z1 = np.dot(W1.T, X) + b1\n",
    "    A1 = np.maximum(Z1, 0)\n",
    "    Z2 = np.dot(W2.T, A1) + b2\n",
    "    Yhat = softmax(Z2)\n",
    "    \n",
    "    # backpropagation\n",
    "    E2 = (Yhat - Y )/N\n",
    "    dW2 = np.dot(A1, E2.T)\n",
    "    db2 = np.sum(E2, axis = 1, keepdims = True)\n",
    "    E1 = np.dot(W2, E2)\n",
    "    E1[Z1 <= 0] = 0 # gradient of ReLU\n",
    "    dW1 = np.dot(X, E1.T)\n",
    "    db1 = np.sum(E1, axis = 1, keepdims = True)\n",
    "\n",
    "    # Gradient Descent update\n",
    "    W1 += -eta*dW1\n",
    "    b1 += -eta*db1\n",
    "    W2 += -eta*dW2\n",
    "    b2 += -eta*db2\n",
    "\n",
    "    # print loss after each 1000 iterations\n",
    "    if i %1 == 0:\n",
    "        # compute the loss: average cross-entropy loss\n",
    "        loss = cost(Y, Yhat)\n",
    "        print(\"iter %d, loss: %f, \" %(i, loss))\n",
    "#         print (dW1[1, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (trainingData1.shape)\n",
    "print (trainingData1[:, 7])\n",
    "print (testLabels.shape)\n",
    "# print (trainingLabels)\n",
    "# print (testLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData1 = trainingData.T\n",
    "print (trainingData1[:, 1])\n",
    "print (trainingData1[:, 90])\n",
    "# print (1/0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
